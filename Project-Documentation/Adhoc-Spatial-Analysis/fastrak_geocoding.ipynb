{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Found credentials at: /Users/jcroff/Library/CloudStorage/Box-Box/dvutils-creds-jcroff.json\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import logging\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from mtcpy.miscio import log_or_print\n",
    "from mtcpy.geospatial import google_geocode_batch\n",
    "\n",
    "user = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(logger_name, output_dir):\n",
    "    \"\"\"Set up a logger with the specified name and output directory.\"\"\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create a file handler for logging\n",
    "    log_file = f\"{output_dir}/{logger_name}.log\"\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    \n",
    "    # Create a formatter and set it for the handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add the handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = pathlib.Path(\n",
    "    f\"/Users/{user}/Library/CloudStorage/Box-Box/DataViz Projects/Data Services/FasTrak Data\"\n",
    ")\n",
    "ft_data = work_dir / \"Fastrak Accounts Cleaned\" / \"Final Geocode Results\" / \"account_complete_9_12_24.csv\"\n",
    "gc_data = work_dir / \"Fastrak Accounts Cleaned\" / \"bay_area_fastrak_accounts_geocoded.csv\"\n",
    "final_gc_data_csv = work_dir / \"Fastrak Accounts Cleaned\" / \"Final Geocode Results\" / \"bay_area_fastrak_accounts_geocoded_final.csv\"\n",
    "final_gc_data_geojson = work_dir / \"Fastrak Accounts Cleaned\" / \"Final Geocode Results\" / \"bay_area_fastrak_accounts_geocoded_final.geojson\"\n",
    "summary_data_xlsx = work_dir / \"Fastrak Accounts Cleaned\" / \"Final Geocode Results\" / \"bay_area_fastrak_accounts_geocoded_summary.xlsx\"\n",
    "final_aggregated_data_csv = work_dir / \"Fastrak Accounts Cleaned\" / \"Final Geocode Results\" / \"bay_area_fastrak_accounts_geocoded_final_aggregated.csv\"\n",
    "final_aggregated_data_geojson = work_dir / \"Fastrak Accounts Cleaned\" / \"Final Geocode Results\" / \"bay_area_fastrak_accounts_geocoded_final_aggregated.geojson\"\n",
    "epc_data = (\n",
    "    \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/\"\n",
    "    \"draft_equity_priority_communities_pba2050plus_acs2022a/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n",
    ")\n",
    "tract_data = (\n",
    "    \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/\"\n",
    "    \"region_2020_censustract/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = setup_logger(logger_name=\"fastrak_geocoding\", output_dir=\"Logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fastrak_data(file_path):\n",
    "    \"\"\"Read the Fastrak data from the specified file path.\"\"\"\n",
    "    log_or_print(f\"Reading Fastrak data from {file_path}\", LOGGER)\n",
    "    df = pd.read_csv(file_path)\n",
    "    log_or_print(f\"Read {len(df)} records from Fastrak data\", LOGGER)\n",
    "\n",
    "    # drop columns that are blank\n",
    "    log_or_print(\"Dropping columns that are blank\", LOGGER)\n",
    "    rm_cols = [col for col in df.columns if col.strip() == '']\n",
    "    df = df.drop(columns=rm_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epc_data(file_path):\n",
    "    \"\"\"Read the EPC data from the specified file path.\"\"\"\n",
    "    log_or_print(f\"Reading EPC data from {file_path}\", LOGGER)\n",
    "    df = gpd.read_file(file_path)\n",
    "    log_or_print(f\"Read {len(df)} records from {file_path}\", LOGGER)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tract_data(file_path):\n",
    "    \"\"\"Read the tract data from the specified file path.\"\"\"\n",
    "    log_or_print(f\"Reading tract data from {file_path}\", LOGGER)\n",
    "    df = gpd.read_file(file_path)\n",
    "    log_or_print(f\"Read {len(df)} records from {file_path}\", LOGGER)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geocoded_data(df, file_path, mode='w', header=True):\n",
    "    \"\"\"Write the geocoded data to the specified file path.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the geocoded data.\n",
    "        file_path (str): Path to write the geocoded data to.\n",
    "        mode (str, optional): Mode to write the data. 'w' for overwrite, 'a' for append. Defaults to 'w'.\n",
    "        header (bool, optional): Whether to write the header. Defaults to True.\n",
    "    \"\"\"\n",
    "    log_or_print(f\"Writing geocoded data to {file_path} with mode={mode} and header={header}\", LOGGER)\n",
    "    df.to_csv(file_path, mode=mode, header=header, index=False)\n",
    "    log_or_print(f\"Wrote {len(df)} records to {file_path}\", LOGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_geocoded_data(ft_gdf, tracts_gdf):\n",
    "    \"\"\"Spatially join the geocoded census tract data.\"\"\"\n",
    "    log_or_print(\n",
    "        f\"Spatially joining geocoded Fastrak data with tract data. Fastrack gdf len: {len(ft_gdf)} Tract gdf len: {len(tracts_gdf)}\",\n",
    "        LOGGER,\n",
    "    )\n",
    "    # check CRS\n",
    "    if ft_gdf.crs != tracts_gdf.crs:\n",
    "        log_or_print(\"CRS do not match. Reprojecting tracts data to match Fastrak data\", LOGGER)\n",
    "        tracts_gdf = tracts_gdf.to_crs(ft_gdf.crs)\n",
    "    \n",
    "    joined_gdf = gpd.sjoin(ft_gdf, tracts_gdf, how=\"left\", predicate=\"intersects\")\n",
    "    log_or_print(f\"Joined {len(joined_gdf)} records\", LOGGER)\n",
    "    return joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_geocoded_data(file_path):\n",
    "    \"\"\"Read the geocoded data from the specified file path.\"\"\"\n",
    "    log_or_print(f\"Reading geocoded data from {file_path}\", LOGGER)\n",
    "    # read in the geocoded data to a GeoDataFrame, which has a geometry column\n",
    "    df = pd.read_csv(gc_data)\n",
    "    g = gpd.GeoSeries.from_wkt(df[\"geometry\"])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=g, crs=\"EPSG:4326\")\n",
    "    log_or_print(f\"Read {len(df)} records from geocoded data\", LOGGER)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_required_cols(df):\n",
    "    \"\"\"Create the required columns for geocoding.\"\"\"\n",
    "    log_or_print(\"Creating required columns for geocoding\", LOGGER)\n",
    "\n",
    "    required_columns = [\"ADDR\", \"CITY\", \"STATE\", \"ZIP_CODE\"]\n",
    "    \n",
    "    # Check if all required columns exist in the DataFrame\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        error_message = f\"Missing required columns: {', '.join(missing_columns)}\"\n",
    "        log_or_print(error_message, LOGGER)\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"FULL_ADDRESS\"] = df[\"ADDR\"] + \", \" + df[\"CITY\"] + \", \" + df[\"STATE\"] + \" \" + df[\"ZIP_CODE\"].astype(str)\n",
    "\n",
    "    log_or_print(\"Created FULL_ADDRESS column\", LOGGER)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_geocode_addresses(df, out_file_path, overwrite_append=None):\n",
    "    \"\"\"Batch geocode addresses using Google Maps Geocoding API.\n",
    "\n",
    "    Function to batch geocode wraps dvutils.geospatial.google_geocode_batch.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the addresses to geocode.\n",
    "        out_file_path (str): Path to write the geocoded data to.\n",
    "        overwrite_append (str, optional): Whether to overwrite the existing geocoded data or append to it. Defaults to None.\n",
    "            Valid values are: overwrite, append\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the geocoded addresses with the following columns:\n",
    "            - address_orig: Original address\n",
    "            - formatted_address: Formatted address\n",
    "            - geometry_location_type: Location type of the geocoded address\n",
    "            - types: Types of the geocoded address\n",
    "            - partial_match: Whether the geocoded address is a partial match\n",
    "            - geometry: Geometric information of the geocoded address\n",
    "    \"\"\"\n",
    "\n",
    "    log_or_print(f\"Starting batch address geocoding on {len(df)} records\", LOGGER)\n",
    "\n",
    "    # If out file exists, and overwrite_local is False, read the geocoded data\n",
    "    if overwrite_append not in [\"overwrite\", \"append\"] and pathlib.Path(out_file_path).exists():\n",
    "        log_or_print(\n",
    "            f\"Local geocoded data file exists at {out_file_path}. Reading local data file\", LOGGER\n",
    "        )\n",
    "        results_df = read_geocoded_data(out_file_path)\n",
    "        log_or_print(f\"Read {len(results_df)} records from geocoded data\", LOGGER)\n",
    "        return results_df\n",
    "    \n",
    "    # check if fastrak data has a geocoded match already, if value is not null, skip geocoding\n",
    "    if \"match\" in df.columns:\n",
    "        df = df[df[\"match\"].isnull()]\n",
    "        log_or_print(f\"Starting geocode on {len(df)} records that do not have a geocode match\", LOGGER)\n",
    "\n",
    "    out_cols = [\n",
    "        \"address_orig\",\n",
    "        \"formatted_address\",\n",
    "        \"geometry_location_type\",\n",
    "        \"types\",\n",
    "        \"partial_match\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        results_df = google_geocode_batch(\n",
    "            address_list=df[\"FULL_ADDRESS\"].tolist(),\n",
    "            include_details=True,\n",
    "            allowed_location_types=[\"ROOFTOP\", \"RANGE_INTERPOLATED\"],\n",
    "        )\n",
    "        log_or_print(f\"Finished batch address geocoding. {len(results_df)} geocoded\", LOGGER)\n",
    "    except Exception as e:\n",
    "        log_or_print(f\"Error during batch geocoding: {e}\", LOGGER)\n",
    "        raise\n",
    "\n",
    "    # check for bad results by checking if bad_addresses.txt file exists\n",
    "    bad_address_file = \"bad_addresses.txt\"\n",
    "    if pathlib.Path(bad_address_file).exists():\n",
    "        log_or_print(f\"Bad addresses file found at {bad_address_file}\", LOGGER)\n",
    "\n",
    "        try:\n",
    "            with open(bad_address_file, \"r\") as f:\n",
    "                bad_addresses = f.read().splitlines()\n",
    "            log_or_print(f\"Found {len(bad_addresses)} bad addresses\", LOGGER)\n",
    "        except Exception as e:\n",
    "            log_or_print(f\"Error reading bad addresses file: {e}\", LOGGER)\n",
    "            raise\n",
    "\n",
    "# write the geocoded data based on the overwrite_append parameter\n",
    "    if overwrite_append == \"append\":\n",
    "        if pathlib.Path(out_file_path).exists():\n",
    "            log_or_print(f\"Appending data to existing file at {out_file_path}\", LOGGER)\n",
    "            existing_df = read_geocoded_data(out_file_path)\n",
    "            combined_df = pd.concat([existing_df, results_df[out_cols]], ignore_index=True)\n",
    "            write_geocoded_data(combined_df, out_file_path, mode=\"w\", header=True)\n",
    "        else:\n",
    "            log_or_print(f\"File does not exist. Creating new file at {out_file_path}\", LOGGER)\n",
    "            write_geocoded_data(results_df[out_cols], out_file_path, mode=\"w\", header=True)\n",
    "    elif overwrite_append == \"overwrite\":\n",
    "        log_or_print(f\"Overwriting existing file at {out_file_path}\", LOGGER)\n",
    "        write_geocoded_data(results_df[out_cols], out_file_path, mode=\"w\", header=True)\n",
    "    \n",
    "    return combined_df if overwrite_append == \"append\" else results_df[out_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the EPC data\n",
    "def join_epc_data(joined_gdf, epc_gdf, epc_cols=[\"geoid\", \"county_fip\", \"epc_2050p\"]):\n",
    "    \"\"\"Join the EPC data to the joined geocoded data.\"\"\"\n",
    "    log_or_print(\n",
    "        f\"Joining the sjoin tract and Fastrak data to EPC data. Sjoin Fastrak data: {len(joined_gdf)} EPC data: {len(epc_gdf)}\",\n",
    "        LOGGER,\n",
    "    )\n",
    "    # rename columns to avoid conflicts\n",
    "    epc_gdf = epc_gdf.rename(columns={\"tract_geoid\": \"geoid\"})\n",
    "    joined_gdf = pd.merge(\n",
    "        joined_gdf, epc_gdf[epc_cols], on=\"geoid\", how=\"left\"\n",
    "    )\n",
    "    log_or_print(f\"Joined {len(joined_gdf)} records\", LOGGER)\n",
    "    return joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to the original ft data\n",
    "def join_original_data(joined_gdf, ft_data):\n",
    "    \"\"\"Join the original Fastrak data to the joined data.\"\"\"\n",
    "    \n",
    "    # check fastrak columns\n",
    "    req_cols = [\n",
    "        \"Is Duplicate Row?\",\n",
    "        \"Table Names\",\n",
    "        \"F4\",\n",
    "        \"ACCTNO\",\n",
    "        \"ACCTTYPE\",\n",
    "        \"ACCTSTATUS\",\n",
    "        \"ADDR\",\n",
    "        \"CITY\",\n",
    "        \"STATE\",\n",
    "        \"ZIP_CODE\",\n",
    "        \"ADDRESS\",\n",
    "        \"TX_DATE\",\n",
    "        \"PLAZA\",\n",
    "        \"TOLLCNT\",\n",
    "        \"VEHCOUNT\",\n",
    "        \"TAGCOUNT\",\n",
    "        \"Jurisdiction\",\n",
    "        \"NAMELSAD20\",\n",
    "        \"FULL_ADDRESS\",\n",
    "    ]\n",
    "\n",
    "    # drop all columns that are not in the required columns\n",
    "    log_or_print(f\"Dropping columns that are not in the required columns list {req_cols}\", LOGGER)\n",
    "    ft_data = ft_data[req_cols].copy()\n",
    "\n",
    "    log_or_print(\n",
    "        f\"Joining the original Fastrak data to the joined data. Joined data: {len(joined_gdf)} Original Fastrak data: {len(ft_data)}\",\n",
    "        LOGGER,\n",
    "    )\n",
    "    # rename ft data address column\n",
    "    ft_data = ft_data.rename(columns={\"FULL_ADDRESS\": \"address_orig\"})\n",
    "    joined_gdf = pd.merge(joined_gdf, ft_data, on=\"address_orig\", how=\"right\")\n",
    "    log_or_print(f\"Joined {len(joined_gdf)} records\", LOGGER)\n",
    "    return joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final geocoding post processing\n",
    "\n",
    "def geocode_post_processing(gdf):\n",
    "    \"\"\"Post processing for geocoded data.\"\"\"\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    # classify geocode accuracy\n",
    "    log_or_print(\n",
    "        \"Flagging matches. Only a match if geometry_location_type in: [ROOFTOP, 'RANGE_INTERPOLATED] and partial_match = False\",\n",
    "        LOGGER,\n",
    "    )\n",
    "    gdf[\"match\"] = (\n",
    "        (gdf[\"geometry_location_type\"].isin([\"ROOFTOP\", \"RANGE_INTERPOLATED\"]))\n",
    "        & (gdf[\"partial_match\"].isnull())\n",
    "    ).astype(int)\n",
    "    # log true/false counts\n",
    "    log_or_print(f\"Flagged {gdf['match'].value_counts().to_dict()} records as matches\", LOGGER)\n",
    "\n",
    "    # Flag data within the region\n",
    "    log_or_print(\"Flagging data within the region\", LOGGER)\n",
    "    gdf[\"in_region\"] = (gdf[\"index_right\"].notnull()).astype(int)\n",
    "    log_or_print(\n",
    "        f\"Flagged {gdf['in_region'].value_counts().to_dict()} records within the region\", LOGGER\n",
    "    )\n",
    "\n",
    "    # drop records that are not matches and not in region\n",
    "    drop_records = gdf[(gdf[\"match\"] == 0) & (gdf[\"in_region\"] == 0)]\n",
    "    log_or_print(f\"Dropping records that are not matches and not in region. Drop count: {len(drop_records)}\", LOGGER)\n",
    "    gdf = gdf.drop(drop_records.index)\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    drop_cols = [\"geometry_location_type\", \"types\", \"partial_match\", \"index_right\"]\n",
    "    log_or_print(f\"Dropping unnecessary columns: {drop_cols}\", LOGGER)\n",
    "    gdf = gdf.drop(columns=drop_cols)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_geocoding_results(gdf):\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    # Summarize the results for epc_data\n",
    "    log_or_print(\"Summarizing the results for epc_data\", LOGGER)\n",
    "    gdf[\"epc_data\"] = gdf[\"epc_2050p\"].map({0: \"Outside EPC\", 1: \"Within EPC\"})\n",
    "    epc_summary = gdf.groupby(\"epc_data\").size().reset_index(name=\"count\")\n",
    "    epc_pv = epc_summary.pivot_table(index=None, columns=\"epc_data\", values=\"count\").reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # Summarize the results for in_region\n",
    "    gdf[\"in_region_data\"] = gdf[\"in_region\"].map(\n",
    "        {0: \"Outside Region\", 1: \"Within Region\"}\n",
    "    )\n",
    "    region_summary = gdf.groupby(\"in_region_data\").size().reset_index(name=\"count\")\n",
    "    region_pv = region_summary.pivot_table(\n",
    "        index=None, columns=\"in_region_data\", values=\"count\"\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Summarize the results for matched\n",
    "    gdf[\"matched_data\"] = gdf[\"match\"].map({0: \"Not Matched\", 1: \"Matched\"})\n",
    "    matched_summary = gdf.groupby(\"matched_data\").size().reset_index(name=\"count\")\n",
    "    matched_pv = matched_summary.pivot_table(\n",
    "        index=None, columns=\"matched_data\", values=\"count\"\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Concatenate all summaries into a single DataFrame\n",
    "    summary_df = pd.concat([epc_pv, region_pv, matched_pv], axis=1)\n",
    "\n",
    "    # Calculate within epc share, within region share, and matched share\n",
    "    summary_df[\"Within EPC Share\"] = round(summary_df[\"Within EPC\"] / gdf.shape[0], 3)\n",
    "    summary_df[\"Within Region Share\"] = round(summary_df[\"Within Region\"] / gdf.shape[0], 3)\n",
    "    summary_df[\"Matched Share\"] = round(summary_df[\"Matched\"] / gdf.shape[0], 3)\n",
    "\n",
    "    # Reorder columns\n",
    "    col_order = [\n",
    "        \"Outside EPC\",\n",
    "        \"Within EPC\",\n",
    "        \"Within EPC Share\",\n",
    "        \"Outside Region\",\n",
    "        \"Within Region\",\n",
    "        \"Within Region Share\",\n",
    "        \"Not Matched\",\n",
    "        \"Matched\",\n",
    "        \"Matched Share\",\n",
    "    ]\n",
    "    summary_df = summary_df[col_order]\n",
    "\n",
    "    # Format the DataFrame to show thousands separator\n",
    "    formatted_df = summary_df.style.format(\n",
    "        {\n",
    "            \"Outside EPC\": \"{:,.0f}\",\n",
    "            \"Within EPC\": \"{:,.0f}\",\n",
    "            \"Within EPC Share\": \"{:.2%}\",\n",
    "            \"Outside Region\": \"{:,.0f}\",\n",
    "            \"Within Region\": \"{:,.0f}\",\n",
    "            \"Within Region Share\": \"{:.2%}\",\n",
    "            \"Not Matched\": \"{:,.0f}\",\n",
    "            \"Matched\": \"{:,.0f}\",\n",
    "            \"Matched Share\": \"{:.2%}\",\n",
    "        }\n",
    "    )\n",
    "    log_or_print(\"Summarized geocoding results\", LOGGER)\n",
    "    return formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final post processing\n",
    "\n",
    "def final_post_processing(gdf):\n",
    "    \"\"\"Final post processing for the joined data.\"\"\"\n",
    "    \n",
    "    # update epc_2050p, match, and in_region columns so they are not null\n",
    "    cols = [\"epc_2050p\", \"match\", \"in_region\"]\n",
    "    log_or_print(f\"Updating columns {cols} to not null\", LOGGER)\n",
    "    gdf[cols] = gdf[cols].fillna(0).astype(int)\n",
    "\n",
    "    # move geometry column to the end\n",
    "    log_or_print(\"Moving geometry column to the end\", LOGGER)\n",
    "    gdf = gdf[[col for col in gdf.columns if col != \"geometry\"] + [\"geometry\"]].copy()\n",
    "\n",
    "    # provide log summary statistics\n",
    "    log_or_print(f\"Fastrak data geocoded results: {gdf['match'].value_counts().to_dict()}\", LOGGER)\n",
    "    log_or_print(f\"Fastrak data in region results: {gdf['in_region'].value_counts().to_dict()}\", LOGGER)\n",
    "    log_or_print(f\"Fastrak data EPC results: {gdf['epc_2050p'].value_counts().to_dict()}\", LOGGER)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write final data to geojson and csv\n",
    "\n",
    "def write_final_data(gdf, out_path_csv, out_path_geojson):\n",
    "    \"\"\"Write the final data to the specified file path.\"\"\"\n",
    "    # write the final geocoded data to geojson file\n",
    "    log_or_print(f\"Writing final geocoded data to {out_path_geojson} excluding records without geometry.\", LOGGER)\n",
    "    geom_gdf = gdf[gdf[\"geometry\"].notnull()].copy()\n",
    "    geom_gdf.to_file(out_path_geojson, driver=\"GeoJSON\")\n",
    "    log_or_print(f\"Wrote {len(geom_gdf)} records to {out_path_geojson}\", LOGGER)\n",
    "\n",
    "    # write the final geocoded data to csv file\n",
    "    log_or_print(f\"Writing final geocoded data to {out_path_csv} including all records\", LOGGER)\n",
    "    # remove geometry column\n",
    "    log_or_print(\"Removing geometry column\", LOGGER)\n",
    "    tabular_df = gdf.drop(columns=\"geometry\").copy()\n",
    "    tabular_df.to_csv(out_path_csv, index=False)\n",
    "    log_or_print(f\"Wrote {len(gdf)} records to {out_path_csv}\", LOGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_data(df, out_path):\n",
    "    \"\"\"Write the summary data to the specified file path.\"\"\"\n",
    "    # write the summary to an Excel file with the same name as the output file, retaining the df style formatting\n",
    "    log_or_print(f\"Writing summary to {out_path}\", LOGGER)\n",
    "    df.to_excel(out_path, index=False)\n",
    "    log_or_print(f\"Wrote summary to {out_path}\", LOGGER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_aggregate_data(gdf, out_path_csv, out_path_geojson):\n",
    "    \"\"\"Write the aggregated data to the specified file path.\"\"\"\n",
    "    # write the final geocoded data to csv file\n",
    "    log_or_print(f\"Writing aggregated data to {out_path_geojson} of length {len(gdf)}\", LOGGER)\n",
    "    gdf.to_file(out_path_geojson, driver=\"GeoJSON\")\n",
    "    log_or_print(f\"Wrote {len(gdf)} records to {out_path_geojson}\", LOGGER)\n",
    "\n",
    "    # write the final geocoded data to csv file\n",
    "    log_or_print(f\"Writing aggregated data to {out_path_csv} of length {len(gdf)}\", LOGGER)\n",
    "    # remove geometry column\n",
    "    tabular_df = gdf.drop(columns=\"geometry\").copy()\n",
    "    tabular_df.to_csv(out_path_csv, index=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to aggregate the gdf to the tract level\n",
    "def aggregate_to_tract(gdf, epc_gdf):\n",
    "    \"\"\"Aggregate the geocoded data to the tract level.\"\"\"\n",
    "    gdf = gdf.copy()\n",
    "    log_or_print(\"Aggregating geocoded data to the tract level\", LOGGER)\n",
    "\n",
    "    # filter out records with match = 0 and in_region = 0 and null geometry\n",
    "    log_or_print(\"Filtering out records with match = 0 and in_region = 0 and null geometry\", LOGGER)\n",
    "    gdf = gdf[(gdf[\"match\"] == 1) & (gdf[\"in_region\"] == 1) & gdf[\"geometry\"].notnull()]\n",
    "\n",
    "    # group by tract and sum the match, in_region, and epc_2050p columns\n",
    "    tract_gdf = (\n",
    "        gdf.groupby([\"geoid\", \"epc_2050p\", \"match\", \"in_region\"]).size().reset_index(name=\"count\")\n",
    "    )\n",
    "    log_or_print(f\"Aggregated to {len(tract_gdf)} records\", LOGGER)\n",
    "\n",
    "    # join the EPC data on geometry column only to get the EPC data\n",
    "    epc_gdf = epc_gdf.rename(columns={\"tract_geoid\": \"geoid\"})\n",
    "    log_or_print(\n",
    "        f\"Joining aggregated data to EPC geodata: {len(tract_gdf)} EPC data: {len(epc_gdf)}\", LOGGER\n",
    "    )\n",
    "    tract_gdf = pd.merge(\n",
    "        epc_gdf[[\"geoid\", \"county_fip\", \"geometry\"]], tract_gdf, on=\"geoid\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return tract_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/xt2lctm54xq6fd45m1lmgp4m0000gp/T/ipykernel_88391/2473920605.py:4: DtypeWarning: Columns (19,20,26,27,29,30,31,32,33,35,36,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # read in the data\n",
    "    ft_df = read_fastrak_data(ft_data)\n",
    "\n",
    "    # create the required columns\n",
    "    ft_df = create_required_cols(ft_df)\n",
    "\n",
    "    # drop duplicated addresses\n",
    "    log_or_print(\n",
    "        f\"Dropping {ft_df.duplicated(subset=['FULL_ADDRESS']).sum()} duplicated addresses\", LOGGER\n",
    "    )\n",
    "    ft_dedup_df = ft_df.drop_duplicates(subset=[\"FULL_ADDRESS\"])\n",
    "\n",
    "    # geocode the addresses\n",
    "    results_gdf = batch_geocode_addresses(\n",
    "        df=ft_dedup_df, out_file_path=gc_data, overwrite_append=None\n",
    "    )\n",
    "\n",
    "    # read the tract data\n",
    "    tract_gdf = read_tract_data(tract_data)\n",
    "\n",
    "    # read the EPC data\n",
    "    epc_gdf = read_epc_data(epc_data)\n",
    "\n",
    "    # spatially join the geocoded data with the tract data\n",
    "    joined_gdf = sjoin_geocoded_data(results_gdf, tract_gdf[[\"geoid\", \"geometry\"]])\n",
    "\n",
    "    # join the EPC data\n",
    "    joined_gdf = join_epc_data(joined_gdf, epc_gdf)\n",
    "\n",
    "    # geocode post processing\n",
    "    joined_gdf = geocode_post_processing(joined_gdf)\n",
    "\n",
    "    # drop duplicates from joined_gdf\n",
    "    log_or_print(\"Dropping duplicates from geocoded data\", LOGGER)\n",
    "    joined_gdf = joined_gdf.drop_duplicates(subset=[\"address_orig\"], keep=\"first\")\n",
    "\n",
    "    # join to the original ft data\n",
    "    final_gdf = join_original_data(joined_gdf, ft_df)\n",
    "\n",
    "    # final post processing\n",
    "    final_gdf = final_post_processing(final_gdf)\n",
    "\n",
    "    # write the final data to geojson and csv\n",
    "    write_final_data(final_gdf, final_gc_data_csv, final_gc_data_geojson)\n",
    "\n",
    "    # summarize the geocoding results\n",
    "    summary_df = summarize_geocoding_results(final_gdf)\n",
    "\n",
    "    # write summary data\n",
    "    write_summary_data(summary_df, summary_data_xlsx)\n",
    "\n",
    "    # aggregate to the tract level\n",
    "    tract_gdf = aggregate_to_tract(final_gdf, epc_gdf)\n",
    "\n",
    "    # write the aggregated data\n",
    "    write_aggregate_data(tract_gdf, final_aggregated_data_csv, final_aggregated_data_geojson)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_gdf = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
