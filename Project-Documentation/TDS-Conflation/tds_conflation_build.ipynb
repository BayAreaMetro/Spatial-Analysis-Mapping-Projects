{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travel Diary Survey Conflation\n",
    "\n",
    "Notebook for Exploratory Data Analysis (EDA) only. Final code will be refactored into a python utilities module and script to run the conflation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this notebook\n",
    "\n",
    "1. Install the required package dependencies using the [requirements.txt](requirements/requirements.txt) file.\n",
    "2. Clone the mappymatch github repository, which has been forked and modified from the original repository.\n",
    "3. Update the MAPPYMATCH_PATH below to point to the location of the cloned repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import getpass\n",
    "\n",
    "user = getpass.getuser().lower()\n",
    "\n",
    "MAPPYMATCH_PATH = f\"/Users/{user}/Documents/GitHub/mappymatch\"\n",
    "sys.path.insert(0, MAPPYMATCH_PATH)\n",
    "\n",
    "from mappymatch import package_root\n",
    "from mappymatch.constructs.trace import Trace\n",
    "from mappymatch.constructs.geofence import Geofence\n",
    "from mappymatch.matchers.lcss.lcss import LCSSMatcher\n",
    "from mappymatch.maps.nx.nx_map import NxMap, NetworkType\n",
    "\n",
    "from mappymatch.utils.plot import plot_trace\n",
    "from mappymatch.utils.plot import plot_map\n",
    "from mappymatch.utils.plot import plot_matches\n",
    "from mappymatch.utils.plot import plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch process function to create a list of traces\n",
    "\n",
    "def create_batch_traces(df, trip_id_column, xy=True):\n",
    "    \"\"\"Create a batch of traces from a dataframe with xy coordinates\n",
    "\n",
    "    Args:\n",
    "        df (Pandas Dataframe): Dataframe with xy coordinates in EPGS:4326.\n",
    "        trip_id_column (String): Column name with unique trip ids.\n",
    "        xy (bool, optional): Projects trace to EPSG:3857. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List: List of dictionaries with trip_id, trace, trace_gdf, and trace_line_gdf.\n",
    "        Structure of the list:\n",
    "        [\n",
    "            {\n",
    "                \"trip_id\": \"unique_id\",\n",
    "                \"trace\": Trace object,\n",
    "                \"trace_gdf\": GeoDataFrame with trace points,\n",
    "                \"trace_line_gdf\": GeoDataFrame with trace line\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    from shapely.geometry import LineString\n",
    "\n",
    "    unique_ids = df[trip_id_column].unique()\n",
    "    batch_traces = []\n",
    "    for i in unique_ids:\n",
    "        filter_df = df[df[\"trip_id\"] == i]\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            filter_df, geometry=gpd.points_from_xy(filter_df.lon, filter_df.lat), crs=4326\n",
    "        )\n",
    "        batch_trace = Trace.from_geo_dataframe(frame=gdf, xy=xy)\n",
    "\n",
    "        # create a trace_line_gdf from the trace\n",
    "        coords = [(p.x, p.y) for p in batch_trace.coords]\n",
    "        line = LineString(coords)\n",
    "        trace_line_gdf = gpd.GeoDataFrame([{\"geometry\": line}], crs=\"EPSG:3857\")\n",
    "        trace_line_gdf[\"trip_id\"] = i\n",
    "\n",
    "        # create a trace_gdf from the trace\n",
    "        trace_gdf = batch_trace._frame\n",
    "        trace_gdf[\"trip_id\"] = i\n",
    "\n",
    "        # create a dictionary with the trip_id, trace, trace_gdf, and trace_line_gdf and append to the batch_traces list\n",
    "        trace_dict = {\n",
    "            \"trip_id\": i,\n",
    "            \"trace\": batch_trace,\n",
    "            \"trace_gdf\": trace_gdf,\n",
    "            \"trace_line_gdf\": trace_line_gdf,\n",
    "        }\n",
    "        batch_traces.append(trace_dict)\n",
    "    return batch_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of traces and batch processes them using the LCSS matcher\n",
    "\n",
    "\n",
    "def batch_process_traces(traces, geofence_buffer=1000, network_type=NetworkType.DRIVE):\n",
    "    \"\"\"Batch process traces using the LCSS matcher.\n",
    "\n",
    "    The function creates a geofence around each trace and creates a networkx graph from the geofence.\n",
    "    Returns a list of matched traces.\n",
    "\n",
    "    Args:\n",
    "        traces (List): list of dictionaries with trip_id and trace.\n",
    "        geofence_buffer (int, optional): Buffer in meters. Defaults to 100.\n",
    "        network_type (Enumerator, optional): Enumerator for Network Types supported by osmnx. Defaults to NetworkType.DRIVE.\n",
    "\n",
    "    Returns:\n",
    "        List: List of dictionaries with trip_id, trace, matched_result, matched_gdf, and matched_path_gdf.\n",
    "        Structure of the list:\n",
    "        [\n",
    "            {\n",
    "            \"trip_id\": trip_id,\n",
    "            \"trace\": trace,\n",
    "            \"unmatched_trips\": None or trip_id,\n",
    "            \"trace_gdf\": trace_gdf,\n",
    "            \"trace_line_gdf\": trace_line_gdf,\n",
    "            \"matched_result\": match_result,\n",
    "            \"matched_gdf\": matched_gdf,\n",
    "            \"matched_path_gdf\": matched_path_gdf,\n",
    "            },\n",
    "        ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    import osmnx as ox\n",
    "    import networkx as nx\n",
    "    import warnings\n",
    "\n",
    "    processed_trip_count = 0\n",
    "    matched_traces = []\n",
    "    for trace_dict in traces:\n",
    "        try:\n",
    "            # create a geofence around the trace\n",
    "            geofence = Geofence.from_trace(trace_dict[\"trace\"], padding=geofence_buffer)\n",
    "\n",
    "            # create a networkx map from the geofence\n",
    "            nx_map = NxMap.from_geofence(geofence, network_type=network_type)\n",
    "\n",
    "            # match the trace to the map\n",
    "            matcher = LCSSMatcher(nx_map)\n",
    "            match_result = matcher.match_trace(trace_dict[\"trace\"])\n",
    "\n",
    "            # add full match result to the trace dictionary\n",
    "            trace_dict[\"matched_result\"] = match_result\n",
    "            matched_traces.append(trace_dict)\n",
    "        except Exception as e:\n",
    "            warnings.warn(\n",
    "                f\"The trace with trip_id {trace_dict['trip_id']} encountered an exception: {e}. Adding trip to the unmatched list.\"\n",
    "            )\n",
    "            trace_dict[\"unmatched_trips\"] = trace_dict[\"trip_id\"]\n",
    "            matched_traces.append(trace_dict)\n",
    "            continue\n",
    "\n",
    "        # check if any road ids within a list of matches are null\n",
    "        road_id_check = True\n",
    "        for match in match_result.matches:\n",
    "            if match.road is None:\n",
    "                road_id_check = False\n",
    "                break\n",
    "\n",
    "        if road_id_check == False:\n",
    "            warnings.warn(\n",
    "                f\"The trace with trip_id {trace_dict['trip_id']} has null road_ids meaning there was no match to the network. Adding to the unmatched list.\"\n",
    "            )\n",
    "            trace_dict[\"unmatched_trips\"] = trace_dict[\"trip_id\"]\n",
    "        else:\n",
    "            # create a geodataframe from the matches and add the trip_id; add the match result and matched df to the trace dictionary\n",
    "\n",
    "            # print(trace_dict[\"trip_id\"]) # debugging\n",
    "            matched_df = match_result.matches_to_dataframe()\n",
    "            matched_df[\"trip_id\"] = trace_dict[\"trip_id\"]\n",
    "            matched_df[\"road_id\"] = matched_df[\"road_id\"]\n",
    "            matched_gdf = gpd.GeoDataFrame(matched_df, geometry=\"geom\", crs=\"EPSG:3857\")\n",
    "\n",
    "            # create a geodataframe from the matched path and add the trip_id; add the match result and matched df to the trace dictionary\n",
    "            matched_path_df = match_result.path_to_dataframe()\n",
    "            matched_path_df[\"trip_id\"] = trace_dict[\"trip_id\"]\n",
    "            matched_path_df[\"road_id\"] = matched_path_df[\"road_id\"]\n",
    "            matched_path_gdf = gpd.GeoDataFrame(matched_path_df, geometry=\"geom\", crs=\"EPSG:3857\")\n",
    "\n",
    "            # add network attributes to the matched gdf and matched path gdf\n",
    "            attrs = [\"ref\", \"name\", \"maxspeed\", \"highway\", \"bridge\", \"tunnel\"]\n",
    "            for attr in attrs:\n",
    "                # get attributes from the raw graph\n",
    "                attr_dict = nx.get_edge_attributes(nx_map.g, attr)\n",
    "                # add attributes to the matched gdf\n",
    "                matched_gdf[attr] = matched_gdf[\"road_id\"].map(attr_dict)\n",
    "                # add attributes to the matched path gdf\n",
    "                matched_path_gdf[attr] = matched_path_gdf[\"road_id\"].map(attr_dict)\n",
    "\n",
    "            # Set unmatched_trips to None and add matched_gdf and matched_path_gdf to the trace dictionary\n",
    "            trace_dict[\"unmatched_trips\"] = None\n",
    "            trace_dict[\"matched_gdf\"] = matched_gdf\n",
    "            trace_dict[\"matched_path_gdf\"] = matched_path_gdf\n",
    "        # processed_trip_count += 1\n",
    "        # print(f\"Processed {processed_trip_count} trips.\")\n",
    "\n",
    "    return matched_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trace(trace_dict, geofence_buffer, network_type):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        trace_dict (_type_): _description_\n",
    "        geofence_buffer (_type_): _description_\n",
    "        network_type (_type_): _description_\n",
    "    \"\"\"\n",
    "    # create a function that takes a single trace and processes it using the LCSS matcher\n",
    "\n",
    "    import osmnx as ox\n",
    "    import networkx as nx\n",
    "    import warnings\n",
    "\n",
    "    # processed_trip_count = 0\n",
    "    # matched_traces = []\n",
    "    try:\n",
    "        # create a geofence around the trace\n",
    "        geofence = Geofence.from_trace(trace_dict[\"trace\"], padding=geofence_buffer)\n",
    "        # create a networkx map from the geofence\n",
    "        nx_map = NxMap.from_geofence(geofence, network_type=network_type)\n",
    "        # match the trace to the map\n",
    "        matcher = LCSSMatcher(nx_map)\n",
    "        match_result = matcher.match_trace(trace_dict[\"trace\"])\n",
    "        # add full match result to the trace dictionary\n",
    "        trace_dict[\"matched_result\"] = match_result\n",
    "        # matched_traces.append(trace_dict)\n",
    "    except Exception as e:\n",
    "        warnings.warn(\n",
    "            f\"The trace with trip_id {trace_dict['trip_id']} encountered an exception: {e}. Adding trip to the unmatched list.\"\n",
    "        )\n",
    "        trace_dict[\"unmatched_trips\"] = trace_dict[\"trip_id\"]\n",
    "        return trace_dict\n",
    "    # check if any road ids within a list of matches are null\n",
    "    road_id_check = True\n",
    "    for match in match_result.matches:\n",
    "        if match.road is None:\n",
    "            road_id_check = False\n",
    "            break\n",
    "    if road_id_check == False:\n",
    "        warnings.warn(\n",
    "            f\"The trace with trip_id {trace_dict['trip_id']} has null road_ids meaning there was no match to the network. Adding to the unmatched list.\"\n",
    "        )\n",
    "        trace_dict[\"unmatched_trips\"] = trace_dict[\"trip_id\"]\n",
    "    else:\n",
    "        # create a geodataframe from the matches and add the trip_id; add the match result and matched df to the trace dictionary\n",
    "        # print(trace_dict[\"trip_id\"]) # debugging\n",
    "        matched_df = match_result.matches_to_dataframe()\n",
    "        matched_df[\"trip_id\"] = trace_dict[\"trip_id\"]\n",
    "        matched_df[\"road_id\"] = matched_df[\"road_id\"]\n",
    "        matched_gdf = gpd.GeoDataFrame(matched_df, geometry=\"geom\", crs=\"EPSG:3857\")\n",
    "        # create a geodataframe from the matched path and add the trip_id; add the match result and matched df to the trace dictionary\n",
    "        matched_path_df = match_result.path_to_dataframe()\n",
    "        matched_path_df[\"trip_id\"] = trace_dict[\"trip_id\"]\n",
    "        matched_path_df[\"road_id\"] = matched_path_df[\"road_id\"]\n",
    "        matched_path_gdf = gpd.GeoDataFrame(matched_path_df, geometry=\"geom\", crs=\"EPSG:3857\")\n",
    "        # add network attributes to the matched gdf and matched path gdf\n",
    "        attrs = [\"ref\", \"name\", \"maxspeed\", \"highway\", \"bridge\", \"tunnel\"]\n",
    "        for attr in attrs:\n",
    "            # get attributes from the raw graph\n",
    "            attr_dict = nx.get_edge_attributes(nx_map.g, attr)\n",
    "            # add attributes to the matched gdf\n",
    "            matched_gdf[attr] = matched_gdf[\"road_id\"].map(attr_dict)\n",
    "            # add attributes to the matched path gdf\n",
    "            matched_path_gdf[attr] = matched_path_gdf[\"road_id\"].map(attr_dict)\n",
    "        # Set unmatched_trips to None and add matched_gdf and matched_path_gdf to the trace dictionary\n",
    "        trace_dict[\"unmatched_trips\"] = None\n",
    "        trace_dict[\"matched_gdf\"] = matched_gdf\n",
    "        trace_dict[\"matched_path_gdf\"] = matched_path_gdf\n",
    "\n",
    "    return trace_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_traces_parallel(traces, geofence_buffer=1000, network_type=NetworkType.DRIVE):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        traces (_type_): _description_\n",
    "        geofence_buffer (int, optional): _description_. Defaults to 1000.\n",
    "        network_type (_type_, optional): _description_. Defaults to NetworkType.DRIVE.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    import concurrent.futures\n",
    "    matched_traces = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        # Prepare future tasks\n",
    "        futures = [executor.submit(process_trace, trace, geofence_buffer, network_type) for trace in traces]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            matched_traces.append(future.result())\n",
    "    return matched_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of dictionaries with matched trace geodataframes, concatenates them, and returns a single geodataframe\n",
    "\n",
    "def concatenate_matched_gdfs(matched_traces, match_type=\"matched_gdf\"):\n",
    "    \"\"\"Concatenate matched trace geodataframes into a single geodataframe.\n",
    "\n",
    "    Args:\n",
    "        matched_traces (List): List of dictionaries with matched trace geodataframes.\n",
    "        match_type (String, optional): Type of match to concatenate. Defaults to \"matched_gdf\". \n",
    "        Options are \"matched_gdf\", \"matched_path_gdf\", \"trace_gdf\".\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Concatenated geodataframe.\n",
    "    \"\"\"\n",
    "    matched_gdfs = []\n",
    "    for trace_dict in matched_traces:\n",
    "        # check if the match type is in the trace dictionary\n",
    "        if match_type not in list(trace_dict.keys()):\n",
    "            print(f\"Match type {match_type} not found in trace dictionary. Skipping.\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Match type {match_type} found in trace dictionary.\")\n",
    "            matched_gdfs.append(trace_dict[match_type])\n",
    "    matched_gdf = pd.concat(matched_gdfs)\n",
    "\n",
    "    # if values in the matched_gdf are lists, convert to strings\n",
    "    for col in matched_gdf.columns:\n",
    "        if matched_gdf[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            matched_gdf[col] = matched_gdf[col].apply(lambda x: \"; \".join(x) if isinstance(x, list) else x)\n",
    "    return matched_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to concatenate matched gdfs and write each match type to geopackage\n",
    "\n",
    "def write_matched_gdfs(match_result, file_path):\n",
    "    \"\"\"Write traces matched with the LCSS matcher to a geopackage.\n",
    "\n",
    "    Args:\n",
    "        match_result (List): List of dictionaries with matched trace geodataframes.\n",
    "        file_path (String): path to the geopackage file. \n",
    "    \"\"\"\n",
    "    trace_gdf = concatenate_matched_gdfs(match_result, match_type=\"trace_gdf\")\n",
    "    trace_line_gdf = concatenate_matched_gdfs(match_result, match_type=\"trace_line_gdf\")\n",
    "    matched_gdf = concatenate_matched_gdfs(match_result, match_type=\"matched_gdf\")\n",
    "    matched_path_gdf = concatenate_matched_gdfs(match_result, match_type=\"matched_path_gdf\") \n",
    "\n",
    "    # convert matched_gdf and matched_path_gdf \"road_id\" column from RoadId data type to string\n",
    "    matched_gdf[\"road_id\"] = matched_gdf[\"road_id\"].astype(str)\n",
    "    matched_path_gdf[\"road_id\"] = matched_path_gdf[\"road_id\"].astype(str)\n",
    "\n",
    "    # write the trace_gdf, trace_line_gdf, matched_gdf, and matched_path_gdf to a geopackage\n",
    "    trace_gdf.to_file(file_path, layer=\"trace_gdf\", driver=\"GPKG\")\n",
    "    trace_line_gdf.to_file(file_path, layer=\"trace_line_gdf\", driver=\"GPKG\")\n",
    "    matched_gdf.to_file(file_path, layer=\"matched_gdf\", driver=\"GPKG\")\n",
    "    matched_path_gdf.to_file(file_path, layer=\"matched_path_gdf\", driver=\"GPKG\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define file name\n",
    "location_tbl = 'location.csv'\n",
    "trip_tbl = 'trip.csv'\n",
    "\n",
    "## Define Box System Root Directory\n",
    "box_dir = os.path.join(\n",
    "    \"/Users\", user, \"Library\", \"CloudStorage\", \"Box-Box\"\n",
    "    )\n",
    "\n",
    "## Define BAUS directory on Box for .csv output files\n",
    "file_dir = os.path.join(\n",
    "    box_dir, \"Modeling and Surveys\", \"Surveys\", \"Travel Diary Survey\",\n",
    "    \"Biennial Travel Diary Survey\", \"Data\",'2023', \"Full Unweighted 2023 Dataset\"\n",
    ")\n",
    "\n",
    "location_path = os.path.join(file_dir, location_tbl)\n",
    "trip_path = os.path.join(file_dir, trip_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read location and trip \n",
    "location_df = pd.read_csv(location_path)\n",
    "trip_df = pd.read_csv(trip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge trips with locations\n",
    "trip_locations = pd.merge(\n",
    "    location_df,\n",
    "    trip_df[\n",
    "        [\n",
    "            \"trip_id\",\n",
    "            \"o_in_region\",\n",
    "            \"d_in_region\",\n",
    "            \"mode_type\",\n",
    "            \"mode_1\",\n",
    "            \"mode_2\",\n",
    "            \"mode_3\",\n",
    "            \"mode_4\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"trip_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>collect_time</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>bearing</th>\n",
       "      <th>speed</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>o_in_region</th>\n",
       "      <th>d_in_region</th>\n",
       "      <th>mode_type</th>\n",
       "      <th>mode_1</th>\n",
       "      <th>mode_2</th>\n",
       "      <th>mode_3</th>\n",
       "      <th>mode_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2333407402022</td>\n",
       "      <td>2023-11-02T00:23:43Z</td>\n",
       "      <td>13.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.85270</td>\n",
       "      <td>-122.21255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2333407402022</td>\n",
       "      <td>2023-11-02T00:23:50Z</td>\n",
       "      <td>8.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.85227</td>\n",
       "      <td>-122.21236</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2333407402022</td>\n",
       "      <td>2023-11-02T00:24:04Z</td>\n",
       "      <td>12.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.85163</td>\n",
       "      <td>-122.21239</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2333407402022</td>\n",
       "      <td>2023-11-02T00:24:23Z</td>\n",
       "      <td>8.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.85092</td>\n",
       "      <td>-122.21197</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2333407402022</td>\n",
       "      <td>2023-11-02T00:24:49Z</td>\n",
       "      <td>11.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.85138</td>\n",
       "      <td>-122.21071</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         trip_id          collect_time  accuracy  bearing  speed       lat  \\\n",
       "0  2333407402022  2023-11-02T00:23:43Z      13.0    120.0    4.0  37.85270   \n",
       "1  2333407402022  2023-11-02T00:23:50Z       8.0    175.0    4.0  37.85227   \n",
       "2  2333407402022  2023-11-02T00:24:04Z      12.0    185.0    4.0  37.85163   \n",
       "3  2333407402022  2023-11-02T00:24:23Z       8.0    129.0    4.0  37.85092   \n",
       "4  2333407402022  2023-11-02T00:24:49Z      11.0     73.0    4.0  37.85138   \n",
       "\n",
       "         lon  o_in_region  d_in_region  mode_type  mode_1  mode_2  mode_3  \\\n",
       "0 -122.21255            1            1          2       2     995     995   \n",
       "1 -122.21236            1            1          2       2     995     995   \n",
       "2 -122.21239            1            1          2       2     995     995   \n",
       "3 -122.21197            1            1          2       2     995     995   \n",
       "4 -122.21071            1            1          2       2     995     995   \n",
       "\n",
       "   mode_4  \n",
       "0     995  \n",
       "1     995  \n",
       "2     995  \n",
       "3     995  \n",
       "4     995  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter trips_locations to only include trips with mode 8 in mode_1 or mode_2 or mode_3 or mode_4 columns with origins and destinations in region\n",
    "car_trips = trip_locations[\n",
    "    ((trip_locations[\"mode_type\"].isin([5, 6, 8, 9, 11]))) & (trip_locations[\"o_in_region\"] == 1)\n",
    "    | (trip_locations[\"d_in_region\"] == 1)\n",
    "]\n",
    "\n",
    "unique_trips = car_trips[\"trip_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344664"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_trips.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create batch traces from the test list\n",
    "# test_list = [\n",
    "#     2304076901001, #highway\n",
    "#     2333407402028, #highway\n",
    "#     2304076901002, #highway\n",
    "#     2347455701047, #highway\n",
    "#     # 2333407402031, #might be too long\n",
    "#     2333407402037,\n",
    "#     2333413601001, # issue with the trace - missing geometry\n",
    "# ]\n",
    "# select top 1000 trips from unique trip list \n",
    "test_list = unique_trips[:20]\n",
    "car_trips_test = car_trips[car_trips['trip_id'].isin(test_list)]\n",
    "batch_traces_test = create_batch_traces(car_trips_test, trip_id_column=\"trip_id\", xy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_traces = create_batch_traces(car_trips, trip_id_column=\"trip_id\", xy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match using the LCSS matching algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: found 127 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 150 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 172 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 302 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 326 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 156 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 184 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 2740 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 2740 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 8 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 159 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 1261 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 162 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 1971 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 2042 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 142 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 121 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 184 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 211 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 1178 links with no geometry; creating geometries from the node lat/lon\n"
     ]
    }
   ],
   "source": [
    "match_result_test = batch_process_traces_parallel(batch_traces_test, geofence_buffer=1000, network_type=NetworkType.DRIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: found 184 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 156 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 326 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 302 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 127 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 150 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 2042 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 1971 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 2740 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 2740 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 172 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 1261 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 8 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 159 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 162 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 142 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 121 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 1178 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 184 links with no geometry; creating geometries from the node lat/lon\n",
      "Warning: found 211 links with no geometry; creating geometries from the node lat/lon\n"
     ]
    }
   ],
   "source": [
    "match_result = batch_process_traces(\n",
    "    traces=batch_traces_test, geofence_buffer=1000, network_type=NetworkType.DRIVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "# cProfile.run('batch_process_traces(traces=batch_traces_test, geofence_buffer=1000, network_type=NetworkType.DRIVE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_path = f\"/Users/{user}/Library/CloudStorage/Box-Box/DataViz Projects/Spatial Analysis and Mapping/TDS Conflation/Data\"\n",
    "gpkg_path = os.path.join(out_file_path, \"tds_conflation_results.gpkg\")\n",
    "write_matched_gdfs(match_result, gpkg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_result = batch_process_traces(\n",
    "#     traces=batch_traces, geofence_buffer=1000, network_type=NetworkType.DRIVE\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0351e672468d449b9a2b2a77a6f3fde168803881ee2ea0f06af4f5d40037bc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
