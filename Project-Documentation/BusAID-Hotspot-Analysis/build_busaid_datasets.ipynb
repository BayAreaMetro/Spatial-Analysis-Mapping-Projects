{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from arcgis import GIS\n",
    "from gtfs_functions import Feed\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "DVUTILS_LOCAL_CLONE_PATH = f\"/Users/{user}/Documents/GitHub/dvutils\"\n",
    "sys.path.insert(0, DVUTILS_LOCAL_CLONE_PATH)\n",
    "from utils_io import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "dir = os.path.join(\"/Users\", user, \"Library\", \"CloudStorage\", \"Box-Box\", \"_GIS Analyses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to remove z values from a geometry\n",
    "def remove_z(geom):\n",
    "    \"\"\"\n",
    "    Removes z values from a geometry\n",
    "\n",
    "    Source: https://gist.github.com/rmania/8c88377a5c902dfbc134795a7af538d8?permalink_comment_id=2893099#gistcomment-2893099\n",
    "    \"\"\"\n",
    "    import shapely\n",
    "\n",
    "    return shapely.wkb.loads(shapely.wkb.dumps(geom, output_dimension=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to read kml files by geometry type and return a geodataframe\n",
    "def read_kml_by_geom_type(directory, geom_type):\n",
    "    \"\"\"Read a kml file by geometry type and return a geodataframe\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        path to the kml directory\n",
    "    geom_type : str\n",
    "        one of ['Point', 'LineString', 'Polygon']\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import glob\n",
    "    import fiona\n",
    "\n",
    "    fiona.supported_drivers[\"KML\"] = \"rw\"\n",
    "\n",
    "    # use glob to get all the csv files\n",
    "    # in the folder\n",
    "    if geom_type == \"Point\":\n",
    "        type_string = \"_point\"\n",
    "    elif geom_type == \"LineString\":\n",
    "        type_string = \"_segment\"\n",
    "    else:\n",
    "        type_string = \"_polygon\"\n",
    "\n",
    "    pattern = re.compile(type_string)\n",
    "    file_list = glob.glob(os.path.join(directory, \"*.kml\"))\n",
    "    gdfs = list()\n",
    "\n",
    "    for file in file_list:\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        if pattern.search(file_name):\n",
    "            # read the file\n",
    "            gdf = gpd.read_file(file, driver=\"KML\")\n",
    "            # add the source file name\n",
    "            gdf[\"source_file\"] = file_name\n",
    "            # remove z values\n",
    "            gdf[\"geometry\"] = gdf[\"geometry\"].apply(remove_z)\n",
    "            gdfs.append(gdf)\n",
    "    gdf = pd.concat(gdfs, ignore_index=True)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to overwrite a feature layer\n",
    "def overwrite_published_feature_layer(f_layer_id, geojson_path, client):\n",
    "    \"\"\"Overwrite a published feature layer\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    f_layer_id : str\n",
    "        id of the feature layer to overwrite\n",
    "    geojson_path : str\n",
    "        path to the geojson file\n",
    "    client : authenticated arcgis client\n",
    "        authentication example below:\n",
    "        from arcgis.gis import GIS\n",
    "        password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "        gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)\n",
    "    \"\"\"\n",
    "    from arcgis.features import FeatureLayerCollection\n",
    "\n",
    "    # get the feature layer\n",
    "    host_flayer = client.content.get(f_layer_id)\n",
    "\n",
    "    # create feature layer collection object\n",
    "    f_layer = FeatureLayerCollection.fromitem(host_flayer)\n",
    "    # overwrite the feature layer\n",
    "    f_layer.manager.overwrite(geojson_path)\n",
    "\n",
    "    print(f\"Overwrote hosted feature layer with id: {f_layer_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that publishes a geojson to agol\n",
    "def publish_geojson_to_agol(\n",
    "    geojson_path,\n",
    "    layer_name,\n",
    "    layer_snippet,\n",
    "    tags,\n",
    "    client,\n",
    "    folder=None,\n",
    "    overwrite=False,\n",
    "    f_layer_id=None,\n",
    "):\n",
    "    \"\"\"Publish a geojson to ArcGIS Online\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    geojson_path : str\n",
    "        path to the geojson file\n",
    "    layer_name : str\n",
    "        name of the layer\n",
    "    layer_snippet : str\n",
    "        layer snippet\n",
    "    tags : list\n",
    "        tags as a comma separated string (e.g. \"tag1, tag2, tag3\")\n",
    "    client : authenticated arcgis client\n",
    "        authentication example below:\n",
    "        from arcgis.gis import GIS\n",
    "        password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "        gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)\n",
    "    folder : str\n",
    "        name of the folder to publish to (optional)\n",
    "    overwrite : bool\n",
    "        if True, overwrite existing layer\n",
    "    f_layer_id : str\n",
    "        if overwrite is True, provide the id of the feature layer to overwrite\n",
    "    \"\"\"\n",
    "    if overwrite:\n",
    "        overwrite_published_feature_layer(f_layer_id, geojson_path, client)\n",
    "    else:\n",
    "        # publish the geojson\n",
    "        item_prop = {\n",
    "            \"type\": \"GeoJson\",\n",
    "            \"title\": layer_name,\n",
    "            \"tags\": tags,\n",
    "            \"snippet\": layer_snippet,\n",
    "            \"overwrite\": True,\n",
    "        }\n",
    "        item = client.content.add(item_properties=item_prop, data=geojson_path, folder=folder)\n",
    "\n",
    "        # publish the item\n",
    "        published_item = item.publish(file_type=\"geojson\")\n",
    "\n",
    "        print(f\"Published {layer_name} to ArcGIS Online as {published_item.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flag_column(flag_gdf, original_gdf, original_id_col, out_column):\n",
    "    if flag_gdf.shape[0] != original_gdf.shape[0]:\n",
    "        original_gdf[out_column] = original_gdf[original_id_col].map(\n",
    "            flag_gdf.groupby(original_id_col)[out_column].first()\n",
    "        )\n",
    "    else:\n",
    "        original_gdf[out_column] = original_gdf[original_id_col].map(\n",
    "            flag_gdf.set_index(original_id_col)[out_column]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list_values(row):\n",
    "    row_list = list(row)\n",
    "    row_list.sort()\n",
    "    list_set = set(row_list)\n",
    "    unique_list = list(list_set)\n",
    "    return \"; \".join(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and pre-process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel master list\n",
    "hs_ms_df = pd.read_excel(\n",
    "    os.path.join(dir, \"BusAID Hotspot Master List_112823.xlsx\"), sheet_name=\"hotspot_data_revised\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking feature service layer IDs into 8 chunks\n"
     ]
    }
   ],
   "source": [
    "# read epcs\n",
    "epc_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/communities_of_concern_2020_acs2018/FeatureServer/0\"\n",
    "epc_gdf = pull_geotable_agol(base_url=epc_url, client=gis, reproject_to_analysis_crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking feature service layer IDs into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# read pdas\n",
    "pda_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/priority_development_areas_pba2050/FeatureServer/0\"\n",
    "pda_gdf = pull_geotable_agol(base_url=pda_url, client=gis, reproject_to_analysis_crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null id\n",
    "hs_ms_df = hs_ms_df[~hs_ms_df[\"hotspot_id\"].isnull()].copy()\n",
    "# set id as integer\n",
    "hs_ms_df[\"hotspot_id\"] = hs_ms_df[\"hotspot_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the following hotspots from the analysis:\n",
    "# - Remove hotspots #108, 109, 110 (WestCAT-identified hotspots) and any routes that only pass through these hotspots. These are being addressed via a DPD Forwards project.\n",
    "# - Remove hotspots #82, 84, 85 (SFMTA-identified hotspots) and any routes that only pass through these hotspots. SFMTA has decided to withdraw these from consideration for the BusAID program.\n",
    "\n",
    "hs_ms_df = hs_ms_df[\n",
    "    ~hs_ms_df[\"hotspot_id\"].isin(\n",
    "        [8, 9, 11, 12, 29, 44, 47, 55, 56, 57, 77, 82, 84, 85, 86, 101, 102, 103, 108, 109, 110]\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point and line gdfs\n",
    "point_gdf = read_kml_by_geom_type(os.path.join(dir, \"Spatial Data\", \"January2024\"), \"Point\")\n",
    "line_gdf = read_kml_by_geom_type(os.path.join(dir, \"Spatial Data\", \"January2024\"), \"LineString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read San Jose VTA stops shapefile\n",
    "point_sj_gdf = gpd.read_file(\n",
    "    os.path.join(\n",
    "        dir,\n",
    "        \"Spatial Data\",\n",
    "        \"January2024\",\n",
    "        \"shapefiles\",\n",
    "        \"San_Jose_Sig_BUSAID\",\n",
    "        \"San_Jose_Sig_BUSAID.shp\",\n",
    "    )\n",
    ").to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hotspot id for San Jose VTA points\n",
    "point_sj_gdf[\"hotspot_id\"] = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the id from name string column and add to a new column\n",
    "line_gdf[\"hotspot_id\"] = line_gdf[\"Name\"].str.extract(r\"^\\((\\d+)\\)\", expand=False).astype(int)\n",
    "point_gdf[\"hotspot_id\"] = point_gdf[\"Name\"].str.extract(r\"^\\((\\d+)\\)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat san jose points with other points\n",
    "point_gdf = pd.concat([point_gdf, point_sj_gdf[[\"hotspot_id\", \"geometry\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge master list with point and line gdfs\n",
    "hs_point_gdf = pd.merge(point_gdf[[\"hotspot_id\", \"geometry\"]], hs_ms_df, on=\"hotspot_id\", how=\"inner\")\n",
    "hs_line_gdf = pd.merge(line_gdf[[\"hotspot_id\", \"geometry\"]], hs_ms_df, on=\"hotspot_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots exploding comma separated route ids into multiple rows\n",
    "hs_route_df = hs_ms_df[[\"hotspot_id\", \"agency\", \"transit_routes\"]].copy()\n",
    "\n",
    "# remove whitespace from transit routes\n",
    "hs_route_df[\"transit_routes\"] = hs_route_df[\"transit_routes\"].astype(str).str.replace(\" \", \"\")\n",
    "\n",
    "# split transit routes into list\n",
    "hs_route_df[\"route\"] = hs_route_df.transit_routes.str.split(\",\")\n",
    "\n",
    "# add agency id column to hotspot route dataframe\n",
    "agency_id_dict = {\n",
    "    \"AC Transit\": \"AC\",\n",
    "    \"BART\": \"BA\",\n",
    "    \"CityBus\": \"SR\",\n",
    "    \"County Connection\": \"CC\",\n",
    "    # \"Dixon Readi-Ride\": \"\", # not in transit routes\n",
    "    \"FAST\": \"FS\",\n",
    "    \"Golden Gate Transit\": \"GG\",\n",
    "    \"LAVTA\": \"WH\",\n",
    "    \"Marin Transit\": \"MA\",\n",
    "    \"NVTA\": \"VN\",\n",
    "    \"Petaluma Transit\": \"PE\",\n",
    "    \"SamTrans\": \"SM\",\n",
    "    \"SFMTA\": \"SF\",\n",
    "    \"SolTrans\": \"ST\",\n",
    "    \"Sonoma County Transit\": \"SO\",\n",
    "    \"Tri Delta\": \"3D\",\n",
    "    \"Union City Transit\": \"UC\",\n",
    "    \"VTA\": \"SC\",\n",
    "    \"WestCAT\": \"WC\",\n",
    "}\n",
    "hs_route_df[\"agency_id\"] = hs_route_df[\"agency\"].map(agency_id_dict)\n",
    "\n",
    "# explode the route column into multiple rows\n",
    "hs_route_expode_df = hs_route_df[[\"hotspot_id\", \"agency\", \"agency_id\", \"route\"]].explode(\"route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_route_expode_df[\"route_id\"] = hs_route_expode_df[\"agency_id\"] + \":\" + hs_route_expode_df[\"route\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update route_id for routes that have a different id in the transit routes table\n",
    "route_dict = {\n",
    "    \"BA:Vine29\": \"VN:29\",\n",
    "    \"BA:SolTransG\": \"ST:G\",\n",
    "    \"BA:R\": \"ST:R\",\n",
    "    \"BA:GGT580\": \"GG:580\",\n",
    "    \"BA:704\": \"GG:704\",\n",
    "    \"BA:AC72R\": \"AC:72R\",\n",
    "    \"BA:72\": \"AC:72\",\n",
    "    \"BA:72M\": \"AC:72M\",\n",
    "    \"BA:800\": \"AC:800\",\n",
    "    \"BA:7\": \"AC:7\",\n",
    "    \"BA:76\": \"AC:76\",\n",
    "    \"BA:376\": \"AC:376\",\n",
    "    \"BA:L\": \"AC:L\",\n",
    "    \"BA:WestCATJK\": \"WC:JX\",\n",
    "    \"BA:JPX\": \"WC:JPX\",\n",
    "    \"BA:JL\": \"WC:J\",\n",
    "    \"BA:JR\": \"WC:J\",\n",
    "    \"SR:1\": \"SR:01\",\n",
    "    \"SR:2\": \"SR:02\",\n",
    "    \"SR:2B\": \"SR:02B\",\n",
    "    \"SR:3\": \"SR:03\",\n",
    "    \"SR:5\": \"SR:05\",\n",
    "    \"SR:4\": \"SR:04\",\n",
    "    \"SR:4B\": \"SR:04B\",\n",
    "    \"SR:6\": \"SR:06\",\n",
    "    \"SR:8\": \"SR:08\",\n",
    "    \"SR:9\": \"SR:09\",\n",
    "    \"SR:20\": \"SO:20\",\n",
    "    \"SR:30\": \"SO:30\",\n",
    "    \"SR:34\": \"SO:34\",\n",
    "    \"SR:46\": \"SO:46\",\n",
    "    \"SR:44\": \"SO:44\",\n",
    "    \"SR:48\": \"SO:48\",\n",
    "    \"SR:60\": \"SO:60\",\n",
    "    \"SR:62\": \"SO:62\",\n",
    "    \"SR:72\": \"GG:72\",\n",
    "    # \"SR:95\": None,\n",
    "    \"SR:101\": \"GG:101\",\n",
    "    # \"FS:microtransit\": None,\n",
    "    # \"PE:10\": None,\n",
    "    # \"PE:501\": None,\n",
    "    \"SM:ECROwl\": \"SM:ECRO\",\n",
    "    \"SM:296Owl\": \"SM:296O\",\n",
    "    # \"SF:K\": None,\n",
    "    \"SO:101(GGT)\": \"GG:101\",\n",
    "    # \"SO:95\": None,\n",
    "    \"UC:41\": \"AC:41\",\n",
    "    \"UC:56\": \"AC:56\",\n",
    "    \"UC:97\": \"AC:97\",\n",
    "    \"UC:210\": \"AC:210\",\n",
    "    # \"SC:500\": None,\n",
    "}\n",
    "hs_route_expode_df[\"route_id\"] = hs_route_expode_df[\"route_id\"].replace(route_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process GTFS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read transit routes\n",
    "# transit_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/511_Transit_Routes_Sep_2023/FeatureServer/0\"\n",
    "# transit_gdf = pull_geotable_agol(base_url=transit_url, client=gis, reproject_to_analysis_crs=False)\n",
    "api_key = os.environ.get(\"GTFS_API_KEY\")\n",
    "gtfs_path = (\n",
    "    f\"http://api.511.org/transit/datafeeds?api_key={api_key}&operator_id=RG&historic=2023-11\"\n",
    ")\n",
    "feed = Feed(\n",
    "    gtfs_path,\n",
    "    time_windows=[0, 6, 10, 15, 19, 22, 24],\n",
    "    busiest_date=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading \"routes.txt\".\n",
      "INFO:root:Reading \"agency.txt\".\n",
      "INFO:root:Reading \"shapes.txt\".\n",
      "INFO:root:accessing trips\n",
      "INFO:root:Start date is None. You should either specify a start date or set busiest_date to True.\n",
      "INFO:root:Reading \"trips.txt\".\n",
      "INFO:root:Reading \"stop_times.txt\".\n",
      "INFO:root:_trips is defined in stop_times\n",
      "INFO:root:Reading \"stops.txt\".\n",
      "INFO:root:computing patterns\n"
     ]
    }
   ],
   "source": [
    "# create line frequency and route dataframes\n",
    "# line_freq = feed.lines_freq\n",
    "routes = feed.routes\n",
    "agency = feed.agency\n",
    "shapes = feed.shapes\n",
    "trips = feed.trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes together to get route shapes with agency names\n",
    "st_gdf = pd.merge(\n",
    "    shapes, trips[[\"trip_id\", \"route_id\", \"direction_id\", \"shape_id\"]], on=\"shape_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "str_gdf = pd.merge(\n",
    "    st_gdf,\n",
    "    routes[[\"route_id\", \"agency_id\", \"route_name\", \"route_type\"]],\n",
    "    on=\"route_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "stra_gdf = pd.merge(\n",
    "    str_gdf,\n",
    "    agency[[\"agency_id\", \"agency_name\", \"agency_url\"]],\n",
    "    on=\"agency_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# reorder columns\n",
    "cols = [\n",
    "    \"route_id\",\n",
    "    \"agency_id\",\n",
    "    \"trip_id\",\n",
    "    \"direction_id\",\n",
    "    \"shape_id\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    \"route_type\",\n",
    "    \"agency_url\",\n",
    "    \"geometry\",\n",
    "]\n",
    "route_gdf = stra_gdf[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all expected agency_ids are within the line_freq dataframe\n",
    "expected_agency_ids = [\n",
    "    \"AC\",\n",
    "    \"BA\",\n",
    "    \"SR\",\n",
    "    \"CC\",\n",
    "    \"FS\",\n",
    "    \"GG\",\n",
    "    \"WH\",\n",
    "    \"MA\",\n",
    "    \"VN\",\n",
    "    \"PE\",\n",
    "    \"SM\",\n",
    "    \"SF\",\n",
    "    \"ST\",\n",
    "    \"SO\",\n",
    "    \"3D\",\n",
    "    \"UC\",\n",
    "    \"SC\",\n",
    "    \"WC\",\n",
    "]\n",
    "line_freq_agency_ids = route_gdf[\"agency_id\"].unique().tolist()\n",
    "missing_agency_ids = []\n",
    "for id in expected_agency_ids:\n",
    "    if id not in line_freq_agency_ids:\n",
    "        missing_agency_ids.append(id)\n",
    "missing_agency_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add human readable route type and direction\n",
    "route_type_dict = route_type_dict = {\n",
    "    0: \"Tram, Streetcar, Light Rail\",\n",
    "    1: \"Subway, Metro\",\n",
    "    2: \"Rail\",\n",
    "    3: \"Bus\",\n",
    "    4: \"Ferry\",\n",
    "    5: \"Cable Tram\",\n",
    "    6: \"Aerial Lift\",\n",
    "    7: \"Funicular\",\n",
    "    11: \"Trollybus\",\n",
    "    12: \"Monorail\",\n",
    "}\n",
    "route_gdf[\"route_type_name\"] = route_gdf[\"route_type\"].map(route_type_dict)\n",
    "route_gdf[\"direction_name\"] = route_gdf[\"direction_id\"].map({0: \"Outbound\", 1: \"Inbound\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "cols = [\n",
    "    \"route_id\",\n",
    "    \"agency_id\",\n",
    "    \"direction_id\",\n",
    "    \"route_type\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    # \"route_short_name\",\n",
    "    \"route_type_name\",\n",
    "    \"direction_name\",\n",
    "    # \"window\",\n",
    "    # \"min_per_trip\",\n",
    "    # \"ntrips\",\n",
    "    \"agency_url\",\n",
    "    \"geometry\",\n",
    "]\n",
    "route_gdf = route_gdf[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter transit route data to only include routes with the following conditions:\n",
    "# - route_type = 3 (bus)\n",
    "# - window = \"6:00-10:00\" (AM peak)\n",
    "# - dir_id = \"Inbound\"\n",
    "\n",
    "# bus_gdf = transit_gdf.query(\"route_type == 3 and window == '6:00-10:00' and dir_id == 'Inbound'\").copy().to_crs(\"EPSG:26910\")\n",
    "bus_lr_gdf = (\n",
    "    route_gdf.query(\"route_type_name.isin(['Bus', 'Tram, Streetcar, Light Rail'])\")\n",
    "    .copy()\n",
    "    .to_crs(\"EPSG:26910\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge hotspot spatial and tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the route id with the bus transit gdf\n",
    "hs_route_gdf = pd.merge(\n",
    "    bus_lr_gdf,\n",
    "    hs_route_expode_df,\n",
    "    on=\"route_id\",\n",
    "    how=\"right\",\n",
    "    suffixes=[\"_gtfs\", \"_hotspot\"],\n",
    "    indicator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_merge\n",
       "both          400348\n",
       "right_only         8\n",
       "left_only          0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_route_gdf[\"_merge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SO:46', 'GG:72', 'SR:95', nan, 'FS:microtransit', 'SO:95',\n",
       "       'SC:500', 'ST:NVTA29'], dtype=object)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_route_gdf.query(\"_merge == 'right_only'\")[\"route_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate routes\n",
    "sub_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"route_id\",\n",
    "    \"direction_id\",\n",
    "    \"agency_id_gtfs\",\n",
    "    \"route_type\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    \"route_type_name\",\n",
    "    \"direction_name\",\n",
    "    \"agency_url\",\n",
    "    \"route\",\n",
    "]\n",
    "hs_route_dedup = hs_route_gdf.query(\"_merge == 'both'\").drop_duplicates(subset=sub_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish datasets to ArcGIS Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# checking if the directory demo_folder\n",
    "# exist or not.\n",
    "if not os.path.exists(\"Data\"):\n",
    "    # if the demo_folder directory is not present\n",
    "    # then create it.\n",
    "    os.makedirs(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export features to local directory\n",
    "point_path = os.path.join(\"Data\", \"hs_point_gdf.geojson\")\n",
    "hs_point_gdf.to_file(point_path, driver=\"GeoJSON\")\n",
    "\n",
    "line_path = os.path.join(\"Data\", \"hs_line_gdf.geojson\")\n",
    "hs_line_gdf.to_file(line_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote hosted feature layer with id: b03b3b392d324f2d828eaad56932e93b\n"
     ]
    }
   ],
   "source": [
    "# publish point features to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=point_path,\n",
    "    layer_name=\"BusAID Hotspots - Point (December 2023)\",\n",
    "    layer_snippet=\"BusAID Hotspot Point Dataset\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"b03b3b392d324f2d828eaad56932e93b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote hosted feature layer with id: c31d2e1c793f43c68ad79cc544453fe9\n"
     ]
    }
   ],
   "source": [
    "# publish line features to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=line_path,\n",
    "    layer_name=\"BusAID Hotspots - Line (December 2023)\",\n",
    "    layer_snippet=\"BusAID Hotspot Line Dataset\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"c31d2e1c793f43c68ad79cc544453fe9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop merge\n",
    "hs_route_dedup.drop(columns=[\"_merge\"], inplace=True)\n",
    "\n",
    "# export features to local directory\n",
    "hs_route_path = os.path.join(\"Data\", \"hotspot_bus_routes_gdf.geojson\")\n",
    "hs_route_dedup.to_file(hs_route_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote hosted feature layer with id: 46749db760bc4371b79f7ae223755b15\n"
     ]
    }
   ],
   "source": [
    "# publish routes near lines to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=hs_route_path,\n",
    "    layer_name=\"Hotspot Transit Routes\",\n",
    "    layer_snippet=\"This dataset represents transit routes that run through hotspots.\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"46749db760bc4371b79f7ae223755b15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay hotspot transit routes with epcs\n",
    "epc_cols = [\n",
    "    \"geoid\",\n",
    "    \"tot_pop\",\n",
    "    \"tot_pop_po\",\n",
    "    \"tot_pop_ci\",\n",
    "    \"tot_hh\",\n",
    "    \"tot_fam\",\n",
    "    \"tot_pop_ov\",\n",
    "    \"pop_poc\",\n",
    "    \"pop_over75\",\n",
    "    \"pop_spfam\",\n",
    "    \"pop_lep\",\n",
    "    \"pop_below2\",\n",
    "    \"pop_disabi\",\n",
    "    \"pop_hus_re\",\n",
    "    \"pop_zvhhs\",\n",
    "    \"epc_2050\",\n",
    "    \"epc_class\",\n",
    "    \"geometry\",\n",
    "]\n",
    "hs_epc_gdf = gpd.sjoin(\n",
    "    hs_route_dedup,\n",
    "    epc_gdf[epc_cols].to_crs(\"EPSG:26910\"),\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dixon census tracts and append to hs epc gdf\n",
    "dixon_tracts = [\"06095253402\", \"06095253403\", \"06095253404\"]\n",
    "dixon_epc_gdf = epc_gdf.query(\"geoid.isin(@dixon_tracts)\").to_crs(\"EPSG:26910\").copy()\n",
    "\n",
    "# add hotspot id column\n",
    "dixon_epc_gdf[\"hotspot_id\"] = 35\n",
    "\n",
    "# append dixon epc gdf to hs epc gdf\n",
    "hs_epc_gdf = pd.concat([hs_epc_gdf, dixon_epc_gdf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots deduplicating by id, geoid, epc_2020, and epc_class\n",
    "hs_epc_dedup = hs_epc_gdf.drop_duplicates(\n",
    "    subset=[\"hotspot_id\", \"geoid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode pdas\n",
    "pda_explode_gdf = pda_gdf.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially join hotspot transit routes with pdas\n",
    "hs_pda_gdf = gpd.sjoin(\n",
    "    hs_route_dedup,\n",
    "    pda_explode_gdf[[\"join_key\", \"pda_name\", \"geometry\"]].to_crs(\"EPSG:26910\"),\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flag column to indicate if a hotspot is within a pda\n",
    "hs_pda_gdf[\"pda_flag\"] = np.where(hs_pda_gdf[\"join_key\"].isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots deduplicating by id, join_key, pda_name, and route_id\n",
    "hs_pda_dedup = (\n",
    "    hs_pda_gdf\n",
    "    .sort_values(\"pda_flag\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"hotspot_id\", \"route_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize tracts by hotspot id, epc, and epc category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize tracts by hotspot, epc_2050, and epc_class\n",
    "hs_tract_summary = (\n",
    "    hs_epc_dedup.groupby([\"hotspot_id\", \"epc_class\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total_census_tracts\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column and calculate percentage of total hotspot tracts\n",
    "# hs_tract_summary[\"pct_hs_total_tracts\"] =\n",
    "\n",
    "hs_tract_summary[\"pct_total_hs_tracts\"] = hs_tract_summary.groupby(\"hotspot_id\")[\n",
    "    \"total_census_tracts\"\n",
    "].transform(lambda x: round(x / x.sum(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table so hotspot ids are rows and epc classes are columns\n",
    "epc_melt = hs_tract_summary.melt(\n",
    "    id_vars=[\"hotspot_id\", \"epc_class\"], value_vars=[\"total_census_tracts\", \"pct_total_hs_tracts\"]\n",
    ")\n",
    "\n",
    "# create a new variable column\n",
    "epc_melt[\"new_variable\"] = epc_melt[\"variable\"] + \"_\" + epc_melt[\"epc_class\"]\n",
    "\n",
    "# pivot table so hotspot ids are rows and epc classes are columns\n",
    "hs_tract_pivot = epc_melt.pivot_table(\n",
    "    values=\"value\", index=\"hotspot_id\", columns=\"new_variable\", aggfunc=\"max\"\n",
    ").reset_index()\n",
    "\n",
    "# remove index name\n",
    "hs_tract_pivot = hs_tract_pivot.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "hs_tract_pivot.rename(\n",
    "    columns={\n",
    "        \"pct_total_hs_tracts_High\": \"pct_tracts_high\",\n",
    "        \"pct_total_hs_tracts_Higher\": \"pct_tracts_higher\",\n",
    "        \"pct_total_hs_tracts_Highest\": \"pct_tracts_highest\",\n",
    "        \"pct_total_hs_tracts_NA\": \"pct_tracts_not_epc\",\n",
    "        \"total_census_tracts_High\": \"count_tracts_high\",\n",
    "        \"total_census_tracts_Higher\": \"count_tracts_higher\",\n",
    "        \"total_census_tracts_Highest\": \"count_tracts_highest\",\n",
    "        \"total_census_tracts_NA\": \"count_tracts_not_epc\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add qaqc columns to total census tracts and percent total census tracts\n",
    "pct_cols = [\"pct_tracts_high\", \"pct_tracts_higher\", \"pct_tracts_highest\", \"pct_tracts_not_epc\"]\n",
    "hs_tract_pivot[\"pct_total\"] = hs_tract_pivot[pct_cols].sum(axis=1)\n",
    "\n",
    "count_cols = [\n",
    "    \"count_tracts_high\",\n",
    "    \"count_tracts_higher\",\n",
    "    \"count_tracts_highest\",\n",
    "    \"count_tracts_not_epc\",\n",
    "]\n",
    "hs_tract_pivot[\"total_tracts\"] = hs_tract_pivot[count_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"pct_tracts_high\",\n",
    "    \"pct_tracts_higher\",\n",
    "    \"pct_tracts_highest\",\n",
    "    \"pct_tracts_not_epc\",\n",
    "    \"pct_total\",\n",
    "    \"count_tracts_high\",\n",
    "    \"count_tracts_higher\",\n",
    "    \"count_tracts_highest\",\n",
    "    \"count_tracts_not_epc\",\n",
    "    \"total_tracts\",\n",
    "]\n",
    "hs_tract_pivot = hs_tract_pivot[reorder_cols].copy()\n",
    "\n",
    "# fill na values with 0\n",
    "hs_tract_pivot.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "excel_path = os.path.join(dir, \"BusAID Hotspot Master List_112823.xlsx\")\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_tract_pivot.to_excel(writer, sheet_name=\"hotspot_epc_summary\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize transit routes by hotspot id, and whether they intersect with pdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pda_rt_summary = (\n",
    "    hs_pda_dedup.groupby([\"hotspot_id\", \"pda_flag\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total_routes\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column and calculate the total number of routes that pass through pdas by the total number of routes that pass through hotspots\n",
    "hs_pda_rt_summary[\"pct_total_routes\"] = hs_pda_rt_summary.groupby(\"hotspot_id\")[\n",
    "    \"total_routes\"\n",
    "].transform(lambda x: round(x / x.sum(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test options for rows with only a single hotspot_id\n",
    "pda_melt = hs_pda_rt_summary.melt(id_vars=[\"hotspot_id\", \"pda_flag\"], value_vars=[\"total_routes\", \"pct_total_routes\"])\n",
    "pda_melt[\"new_variable\"] = pda_melt[\"variable\"] + \"_\" + pda_melt[\"pda_flag\"].astype(str)\n",
    "hs_pda_rt_pivot = pda_melt.pivot(index=\"hotspot_id\", columns=\"new_variable\", values=\"value\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pda_rt_pivot.rename(\n",
    "    columns={\n",
    "        \"pct_total_routes_0\": \"pct_routes_outside_pda\",\n",
    "        \"pct_total_routes_1\": \"pct_routes_inside_pda\",\n",
    "        \"total_routes_0\": \"count_routes_outside_pda\",\n",
    "        \"total_routes_1\": \"count_routes_inside_pda\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add qaqc columns to total routes and percent total routes\n",
    "pct_cols = [\"pct_routes_outside_pda\", \"pct_routes_inside_pda\"]\n",
    "hs_pda_rt_pivot[\"pct_total\"] = hs_pda_rt_pivot[pct_cols].sum(axis=1)\n",
    "\n",
    "count_cols = [\"count_routes_outside_pda\", \"count_routes_inside_pda\"]\n",
    "hs_pda_rt_pivot[\"total_routes\"] = hs_pda_rt_pivot[count_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "reorder_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"pct_routes_outside_pda\",\n",
    "    \"pct_routes_inside_pda\",\n",
    "    \"pct_total\",\n",
    "    \"count_routes_outside_pda\",\n",
    "    \"count_routes_inside_pda\",\n",
    "    \"total_routes\",\n",
    "]\n",
    "\n",
    "hs_pda_rt_pivot = hs_pda_rt_pivot[reorder_cols].copy()\n",
    "\n",
    "# fill na values with 0\n",
    "hs_pda_rt_pivot.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index name\n",
    "hs_pda_rt_pivot = hs_pda_rt_pivot.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_pda_rt_pivot.to_excel(writer, sheet_name=\"hotspot_pda_summary\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize demographic data by hotspot id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize tracts by hotspot, epc_2050, and epc_class\n",
    "out_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"geoid\",\n",
    "    \"tot_pop\",\n",
    "    \"tot_pop_po\",\n",
    "    \"tot_pop_ci\",\n",
    "    \"tot_hh\",\n",
    "    \"tot_fam\",\n",
    "    \"tot_pop_ov\",\n",
    "    \"pop_poc\",\n",
    "    \"pop_over75\",\n",
    "    \"pop_spfam\",\n",
    "    \"pop_lep\",\n",
    "    \"pop_below2\",\n",
    "    \"pop_disabi\",\n",
    "    \"pop_hus_re\",\n",
    "    \"pop_zvhhs\",\n",
    "]\n",
    "\n",
    "hs_demo_summary = (\n",
    "    hs_epc_dedup.groupby([\"hotspot_id\"], dropna=False)[out_cols]\n",
    "    .agg(\n",
    "        total_tracts=(\"geoid\", \"count\"),\n",
    "        total_population=(\"tot_pop\", \"sum\"),\n",
    "        total_population_poverty=(\"tot_pop_po\", \"sum\"),\n",
    "        total_population_civilian=(\"tot_pop_ci\", \"sum\"),\n",
    "        total_households=(\"tot_hh\", \"sum\"),\n",
    "        total_families=(\"tot_fam\", \"sum\"),\n",
    "        total_population_over5=(\"tot_pop_ov\", \"sum\"),\n",
    "        pop_people_of_color=(\"pop_poc\", \"sum\"),\n",
    "        pop_seniors_ov75=(\"pop_over75\", \"sum\"),\n",
    "        pop_single_parent_families=(\"pop_spfam\", \"sum\"),\n",
    "        pop_limited_english_prof=(\"pop_lep\", \"sum\"),\n",
    "        pop_low_income=(\"pop_below2\", \"sum\"),\n",
    "        pop_disability=(\"pop_disabi\", \"sum\"),\n",
    "        pop_rent_burdened=(\"pop_hus_re\", \"sum\"),\n",
    "        pop_zero_vehicle_hhs=(\"pop_zvhhs\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate population percentages\n",
    "hs_demo_summary[\"pct_people_of_color\"] = np.where(\n",
    "    hs_demo_summary[\"total_population\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_people_of_color\"] / hs_demo_summary[\"total_population\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_seniors_ov75\"] = np.where(\n",
    "    hs_demo_summary[\"total_population\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_seniors_ov75\"] / hs_demo_summary[\"total_population\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_single_parent_families\"] = np.where(\n",
    "    hs_demo_summary[\"total_families\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_single_parent_families\"] / hs_demo_summary[\"total_families\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_limited_english_prof\"] = np.where(\n",
    "    hs_demo_summary[\"total_population_over5\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_limited_english_prof\"] / hs_demo_summary[\"total_population_over5\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_low_income\"] = np.where(\n",
    "    hs_demo_summary[\"total_population_poverty\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_low_income\"] / hs_demo_summary[\"total_population_poverty\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_disability\"] = np.where(\n",
    "    hs_demo_summary[\"total_population_civilian\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_disability\"] / hs_demo_summary[\"total_population_civilian\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_zero_vehicle_hhs\"] = np.where(\n",
    "    hs_demo_summary[\"total_households\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_zero_vehicle_hhs\"] / hs_demo_summary[\"total_households\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_rent_burdened\"] = np.where(\n",
    "    hs_demo_summary[\"total_households\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_rent_burdened\"] / hs_demo_summary[\"total_households\"]), 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_demo_summary.to_excel(writer, sheet_name=\"hotspot_demographic_summary\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
