{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "DVUTILS_LOCAL_CLONE_PATH = f\"/Users/{user}/Documents/GitHub/dvutils\"\n",
    "sys.path.insert(0, DVUTILS_LOCAL_CLONE_PATH)\n",
    "from utils_io import *\n",
    "\n",
    "from arcgis import GIS\n",
    "from gtfs_functions import Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "dir = os.path.join(\"/Users\", user, \"Library\", \"CloudStorage\", \"Box-Box\", \"_GIS Analyses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to remove z values from a geometry\n",
    "def remove_z(geom):\n",
    "    \"\"\"\n",
    "    Removes z values from a geometry\n",
    "\n",
    "    Source: https://gist.github.com/rmania/8c88377a5c902dfbc134795a7af538d8?permalink_comment_id=2893099#gistcomment-2893099\n",
    "    \"\"\"\n",
    "    import shapely\n",
    "\n",
    "    return shapely.wkb.loads(shapely.wkb.dumps(geom, output_dimension=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to read kml files by geometry type and return a geodataframe\n",
    "def read_kml_by_geom_type(directory, geom_type):\n",
    "    \"\"\"Read a kml file by geometry type and return a geodataframe\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        path to the kml directory\n",
    "    geom_type : str\n",
    "        one of ['Point', 'LineString', 'Polygon']\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import glob\n",
    "    import fiona\n",
    "\n",
    "    fiona.supported_drivers[\"KML\"] = \"rw\"\n",
    "\n",
    "    # use glob to get all the csv files\n",
    "    # in the folder\n",
    "    if geom_type == \"Point\":\n",
    "        type_string = \"_point\"\n",
    "    elif geom_type == \"LineString\":\n",
    "        type_string = \"_segment\"\n",
    "    else:\n",
    "        type_string = \"_polygon\"\n",
    "\n",
    "    pattern = re.compile(type_string)\n",
    "    file_list = glob.glob(os.path.join(directory, \"*.kml\"))\n",
    "    gdfs = list()\n",
    "\n",
    "    for file in file_list:\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        if pattern.search(file_name):\n",
    "            # read the file\n",
    "            gdf = gpd.read_file(file, driver=\"KML\")\n",
    "            # add the source file name\n",
    "            gdf[\"source_file\"] = file_name\n",
    "            # remove z values\n",
    "            gdf[\"geometry\"] = gdf[\"geometry\"].apply(remove_z)\n",
    "            gdfs.append(gdf)\n",
    "    gdf = pd.concat(gdfs, ignore_index=True)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to overwrite a feature layer\n",
    "def overwrite_published_feature_layer(f_layer_id, geojson_path, client):\n",
    "    \"\"\"Overwrite a published feature layer\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    f_layer_id : str\n",
    "        id of the feature layer to overwrite\n",
    "    geojson_path : str\n",
    "        path to the geojson file\n",
    "    client : authenticated arcgis client\n",
    "        authentication example below:\n",
    "        from arcgis.gis import GIS\n",
    "        password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "        gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)\n",
    "    \"\"\"\n",
    "    from arcgis.features import FeatureLayerCollection\n",
    "\n",
    "    # get the feature layer\n",
    "    host_flayer = client.content.get(f_layer_id)\n",
    "\n",
    "    # create feature layer collection object\n",
    "    f_layer = FeatureLayerCollection.fromitem(host_flayer)\n",
    "    # overwrite the feature layer\n",
    "    f_layer.manager.overwrite(geojson_path)\n",
    "\n",
    "    print(f\"Overwrote hosted feature layer with id: {f_layer_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that publishes a geojson to agol\n",
    "def publish_geojson_to_agol(\n",
    "    geojson_path,\n",
    "    layer_name,\n",
    "    layer_snippet,\n",
    "    tags,\n",
    "    client,\n",
    "    folder=None,\n",
    "    overwrite=False,\n",
    "    f_layer_id=None,\n",
    "):\n",
    "    \"\"\"Publish a geojson to ArcGIS Online\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    geojson_path : str\n",
    "        path to the geojson file\n",
    "    layer_name : str\n",
    "        name of the layer\n",
    "    layer_snippet : str\n",
    "        layer snippet\n",
    "    tags : list\n",
    "        tags as a comma separated string (e.g. \"tag1, tag2, tag3\")\n",
    "    client : authenticated arcgis client\n",
    "        authentication example below:\n",
    "        from arcgis.gis import GIS\n",
    "        password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "        gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)\n",
    "    folder : str\n",
    "        name of the folder to publish to (optional)\n",
    "    overwrite : bool\n",
    "        if True, overwrite existing layer\n",
    "    f_layer_id : str\n",
    "        if overwrite is True, provide the id of the feature layer to overwrite\n",
    "    \"\"\"\n",
    "    if overwrite:\n",
    "        overwrite_published_feature_layer(f_layer_id, geojson_path, client)\n",
    "    else:\n",
    "        # publish the geojson\n",
    "        item_prop = {\n",
    "            \"type\": \"GeoJson\",\n",
    "            \"title\": layer_name,\n",
    "            \"tags\": tags,\n",
    "            \"snippet\": layer_snippet,\n",
    "            \"overwrite\": True,\n",
    "        }\n",
    "        item = client.content.add(item_properties=item_prop, data=geojson_path, folder=folder)\n",
    "\n",
    "        # publish the item\n",
    "        published_item = item.publish(file_type=\"geojson\")\n",
    "\n",
    "        print(f\"Published {layer_name} to ArcGIS Online as {published_item.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flag_column(flag_gdf, original_gdf, original_id_col, out_column):\n",
    "    if flag_gdf.shape[0] != original_gdf.shape[0]:\n",
    "        original_gdf[out_column] = original_gdf[original_id_col].map(\n",
    "            flag_gdf.groupby(original_id_col)[out_column].first()\n",
    "        )\n",
    "    else:\n",
    "        original_gdf[out_column] = original_gdf[original_id_col].map(\n",
    "            flag_gdf.set_index(original_id_col)[out_column]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list_values(row):\n",
    "    row_list = list(row)\n",
    "    row_list.sort()\n",
    "    list_set = set(row_list)\n",
    "    unique_list = list(list_set)\n",
    "    return \"; \".join(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel master list\n",
    "hs_ms_df = pd.read_excel(\n",
    "    os.path.join(dir, \"BusAID Hotspot Master List_112823.xlsx\"), sheet_name=\"hotspot_data_revised\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null id\n",
    "hs_ms_df = hs_ms_df[~hs_ms_df[\"hotspot_id\"].isnull()].copy()\n",
    "# set id as integer\n",
    "hs_ms_df[\"hotspot_id\"] = hs_ms_df[\"hotspot_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the following hotspots from the analysis:\n",
    "# - Remove hotspots #108, 109, 110 (WestCAT-identified hotspots) and any routes that only pass through these hotspots. These are being addressed via a DPD Forwards project.\n",
    "# - Remove hotspots #82, 84, 85 (SFMTA-identified hotspots) and any routes that only pass through these hotspots. SFMTA has decided to withdraw these from consideration for the BusAID program.\n",
    "\n",
    "hs_ms_df = hs_ms_df[~hs_ms_df[\"hotspot_id\"].isin([108, 109, 110, 82, 84, 85])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point and line gdfs\n",
    "point_gdf = read_kml_by_geom_type(os.path.join(dir, \"Spatial Data\"), \"Point\")\n",
    "line_gdf = read_kml_by_geom_type(os.path.join(dir, \"Spatial Data\"), \"LineString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the id from name string column and add to a new column\n",
    "line_gdf[\"hotspot_id\"] = line_gdf[\"Name\"].str.extract(r\"^\\((\\d+)\\)\", expand=False).astype(int)\n",
    "point_gdf[\"hotspot_id\"] = point_gdf[\"Name\"].str.extract(r\"^\\((\\d+)\\)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge master list with point and line gdfs\n",
    "hs_point_gdf = pd.merge(point_gdf[[\"hotspot_id\", \"geometry\"]], hs_ms_df, on=\"hotspot_id\", how=\"inner\")\n",
    "hs_line_gdf = pd.merge(line_gdf[[\"hotspot_id\", \"geometry\"]], hs_ms_df, on=\"hotspot_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read transit routes\n",
    "# transit_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/511_Transit_Routes_Sep_2023/FeatureServer/0\"\n",
    "# transit_gdf = pull_geotable_agol(base_url=transit_url, client=gis, reproject_to_analysis_crs=False)\n",
    "api_key = os.environ.get(\"GTFS_API_KEY\")\n",
    "gtfs_path = (\n",
    "    f\"http://api.511.org/transit/datafeeds?api_key={api_key}&operator_id=RG&historic=2023-11\"\n",
    ")\n",
    "feed = Feed(\n",
    "    gtfs_path,\n",
    "    time_windows=[0, 6, 10, 15, 19, 22, 24],\n",
    "    busiest_date=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading \"routes.txt\".\n",
      "INFO:root:Reading \"agency.txt\".\n",
      "INFO:root:Reading \"shapes.txt\".\n",
      "INFO:root:accessing trips\n",
      "INFO:root:Start date is None. You should either specify a start date or set busiest_date to True.\n",
      "INFO:root:Reading \"trips.txt\".\n",
      "INFO:root:Reading \"stop_times.txt\".\n",
      "INFO:root:_trips is defined in stop_times\n",
      "INFO:root:Reading \"stops.txt\".\n",
      "INFO:root:computing patterns\n"
     ]
    }
   ],
   "source": [
    "# create line frequency and route dataframes\n",
    "# line_freq = feed.lines_freq\n",
    "routes = feed.routes\n",
    "agency = feed.agency\n",
    "shapes = feed.shapes\n",
    "trips = feed.trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes together to get route shapes with agency names\n",
    "st_gdf = pd.merge(\n",
    "    shapes, trips[[\"trip_id\", \"route_id\", \"direction_id\", \"shape_id\"]], on=\"shape_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "str_gdf = pd.merge(\n",
    "    st_gdf,\n",
    "    routes[[\"route_id\", \"agency_id\", \"route_name\", \"route_type\"]],\n",
    "    on=\"route_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "stra_gdf = pd.merge(\n",
    "    str_gdf,\n",
    "    agency[[\"agency_id\", \"agency_name\", \"agency_url\"]],\n",
    "    on=\"agency_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# reorder columns\n",
    "cols = [\n",
    "    \"route_id\",\n",
    "    \"agency_id\",\n",
    "    \"trip_id\",\n",
    "    \"direction_id\",\n",
    "    \"shape_id\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    \"route_type\",\n",
    "    \"agency_url\",\n",
    "    \"geometry\",\n",
    "]\n",
    "route_gdf = stra_gdf[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge line frequency and route dataframes\n",
    "# route_freq = pd.merge(\n",
    "#     line_freq,\n",
    "#     routes[[\"route_id\", \"agency_id\", \"route_short_name\", \"route_type\"]],\n",
    "#     on=\"route_id\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "\n",
    "# # merge route frequency with agency\n",
    "# route_freq = pd.merge(\n",
    "#     route_freq, agency[[\"agency_id\", \"agency_name\", \"agency_url\"]], on=\"agency_id\", how=\"left\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all expected agency_ids are within the line_freq dataframe\n",
    "expected_agency_ids = [\n",
    "    \"AC\",\n",
    "    \"BA\",\n",
    "    \"SR\",\n",
    "    \"CC\",\n",
    "    \"FS\",\n",
    "    \"GG\",\n",
    "    \"WH\",\n",
    "    \"MA\",\n",
    "    \"VN\",\n",
    "    \"PE\",\n",
    "    \"SM\",\n",
    "    \"SF\",\n",
    "    \"ST\",\n",
    "    \"SO\",\n",
    "    \"3D\",\n",
    "    \"UC\",\n",
    "    \"SC\",\n",
    "    \"WC\",\n",
    "]\n",
    "line_freq_agency_ids = route_gdf[\"agency_id\"].unique().tolist()\n",
    "missing_agency_ids = []\n",
    "for id in expected_agency_ids:\n",
    "    if id not in line_freq_agency_ids:\n",
    "        missing_agency_ids.append(id)\n",
    "missing_agency_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add human readable route type and direction\n",
    "route_type_dict = route_type_dict = {\n",
    "    0: \"Tram, Streetcar, Light Rail\",\n",
    "    1: \"Subway, Metro\",\n",
    "    2: \"Rail\",\n",
    "    3: \"Bus\",\n",
    "    4: \"Ferry\",\n",
    "    5: \"Cable Tram\",\n",
    "    6: \"Aerial Lift\",\n",
    "    7: \"Funicular\",\n",
    "    11: \"Trollybus\",\n",
    "    12: \"Monorail\",\n",
    "}\n",
    "route_gdf[\"route_type_name\"] = route_gdf[\"route_type\"].map(route_type_dict)\n",
    "route_gdf[\"direction_name\"] = route_gdf[\"direction_id\"].map({0: \"Outbound\", 1: \"Inbound\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "cols = [\n",
    "    \"route_id\",\n",
    "    \"agency_id\",\n",
    "    \"direction_id\",\n",
    "    \"route_type\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    # \"route_short_name\",\n",
    "    \"route_type_name\",\n",
    "    \"direction_name\",\n",
    "    # \"window\",\n",
    "    # \"min_per_trip\",\n",
    "    # \"ntrips\",\n",
    "    \"agency_url\",\n",
    "    \"geometry\",\n",
    "]\n",
    "route_gdf = route_gdf[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking feature service layer IDs into 8 chunks\n"
     ]
    }
   ],
   "source": [
    "# read epcs\n",
    "epc_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/communities_of_concern_2020_acs2018/FeatureServer/0\"\n",
    "epc_gdf = pull_geotable_agol(base_url=epc_url, client=gis, reproject_to_analysis_crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking feature service layer IDs into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# read pdas\n",
    "pda_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/priority_development_areas_pba2050/FeatureServer/0\"\n",
    "pda_gdf = pull_geotable_agol(base_url=pda_url, client=gis, reproject_to_analysis_crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter transit route data to only include routes with the following conditions:\n",
    "# - route_type = 3 (bus)\n",
    "# - window = \"6:00-10:00\" (AM peak)\n",
    "# - dir_id = \"Inbound\"\n",
    "\n",
    "# bus_gdf = transit_gdf.query(\"route_type == 3 and window == '6:00-10:00' and dir_id == 'Inbound'\").copy().to_crs(\"EPSG:26910\")\n",
    "bus_lr_gdf = (\n",
    "    route_gdf.query(\"route_type_name.isin(['Bus', 'Tram, Streetcar, Light Rail'])\")\n",
    "    .copy()\n",
    "    .to_crs(\"EPSG:26910\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots exploding comma separated route ids into multiple rows\n",
    "hs_route_df = hs_ms_df[[\"hotspot_id\", \"agency\", \"transit_routes\"]].copy()\n",
    "\n",
    "# remove whitespace from transit routes\n",
    "hs_route_df[\"transit_routes\"] = hs_route_df[\"transit_routes\"].astype(str).str.replace(\" \", \"\")\n",
    "\n",
    "# split transit routes into list\n",
    "hs_route_df[\"route\"] = hs_route_df.transit_routes.str.split(\",\")\n",
    "\n",
    "# add agency id column to hotspot route dataframe\n",
    "agency_id_dict = {\n",
    "    \"AC Transit\": \"AC\",\n",
    "    \"BART\": \"BA\",\n",
    "    \"CityBus\": \"SR\",\n",
    "    \"County Connection\": \"CC\",\n",
    "    # \"Dixon Readi-Ride\": \"\", # not in transit routes\n",
    "    \"FAST\": \"FS\",\n",
    "    \"Golden Gate Transit\": \"GG\",\n",
    "    \"LAVTA\": \"WH\",\n",
    "    \"Marin Transit\": \"MA\",\n",
    "    \"NVTA\": \"VN\",\n",
    "    \"Petaluma Transit\": \"PE\",\n",
    "    \"SamTrans\": \"SM\",\n",
    "    \"SFMTA\": \"SF\",\n",
    "    \"SolTrans\": \"ST\",\n",
    "    \"Sonoma County Transit\": \"SO\",\n",
    "    \"Tri Delta\": \"3D\",\n",
    "    \"Union City Transit\": \"UC\",\n",
    "    \"VTA\": \"SC\",\n",
    "    \"WestCAT\": \"WC\",\n",
    "}\n",
    "hs_route_df[\"agency_id\"] = hs_route_df[\"agency\"].map(agency_id_dict)\n",
    "\n",
    "# explode the route column into multiple rows\n",
    "hs_route_expode_df = hs_route_df[[\"hotspot_id\", \"agency\", \"agency_id\", \"route\"]].explode(\"route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_route_expode_df[\"route_id\"] = hs_route_expode_df[\"agency_id\"] + \":\" + hs_route_expode_df[\"route\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update route_id for routes that have a different id in the transit routes table\n",
    "route_dict = {\n",
    "    \"BA:Vine29\": \"VN:29\",\n",
    "    \"BA:SolTransG\": \"ST:G\",\n",
    "    \"BA:R\": \"ST:R\",\n",
    "    \"BA:GGT580\": \"GG:580\",\n",
    "    \"BA:704\": \"GG:704\",\n",
    "    \"BA:AC72R\": \"AC:72R\",\n",
    "    \"BA:72\": \"AC:72\",\n",
    "    \"BA:72M\": \"AC:72M\",\n",
    "    \"BA:800\": \"AC:800\",\n",
    "    \"BA:7\": \"AC:7\",\n",
    "    \"BA:76\": \"AC:76\",\n",
    "    \"BA:376\": \"AC:376\",\n",
    "    \"BA:L\": \"AC:L\",\n",
    "    \"BA:WestCATJK\": \"WC:JX\",\n",
    "    \"BA:JPX\": \"WC:JPX\",\n",
    "    \"BA:JL\": \"WC:J\",\n",
    "    \"BA:JR\": \"WC:J\",\n",
    "    \"SR:1\": \"SR:01\",\n",
    "    \"SR:2\": \"SR:02\",\n",
    "    \"SR:2B\": \"SR:02B\",\n",
    "    \"SR:3\": \"SR:03\",\n",
    "    \"SR:5\": \"SR:05\",\n",
    "    \"SR:4\": \"SR:04\",\n",
    "    \"SR:4B\": \"SR:04B\",\n",
    "    \"SR:6\": \"SR:06\",\n",
    "    \"SR:8\": \"SR:08\",\n",
    "    \"SR:9\": \"SR:09\",\n",
    "    \"SR:20\": \"SO:20\",\n",
    "    \"SR:30\": \"SO:30\",\n",
    "    \"SR:34\": \"SO:34\",\n",
    "    \"SR:46\": \"SO:46\",\n",
    "    \"SR:44\": \"SO:44\",\n",
    "    \"SR:48\": \"SO:48\",\n",
    "    \"SR:60\": \"SO:60\",\n",
    "    \"SR:62\": \"SO:62\",\n",
    "    \"SR:72\": \"GG:72\",\n",
    "    # \"SR:95\": None,\n",
    "    \"SR:101\": \"GG:101\",\n",
    "    # \"FS:microtransit\": None,\n",
    "    # \"PE:10\": None,\n",
    "    # \"PE:501\": None,\n",
    "    \"SM:ECROwl\": \"SM:ECRO\",\n",
    "    \"SM:296Owl\": \"SM:296O\",\n",
    "    # \"SF:K\": None,\n",
    "    \"SO:101(GGT)\": \"GG:101\",\n",
    "    # \"SO:95\": None,\n",
    "    \"UC:41\": \"AC:41\",\n",
    "    \"UC:56\": \"AC:56\",\n",
    "    \"UC:97\": \"AC:97\",\n",
    "    \"UC:210\": \"AC:210\",\n",
    "    # \"SC:500\": None,\n",
    "}\n",
    "hs_route_expode_df[\"route_id\"] = hs_route_expode_df[\"route_id\"].replace(route_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the route id with the bus transit gdf\n",
    "hs_route_gdf = pd.merge(\n",
    "    bus_lr_gdf,\n",
    "    hs_route_expode_df,\n",
    "    on=\"route_id\",\n",
    "    how=\"right\",\n",
    "    suffixes=[\"_gtfs\", \"_hotspot\"],\n",
    "    indicator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_merge\n",
       "both          518881\n",
       "right_only         7\n",
       "left_only          0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_route_gdf[\"_merge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SO:46', 'GG:72', 'SR:95', nan, 'FS:microtransit', 'SO:95',\n",
       "       'SC:500'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_route_gdf.query(\"_merge == 'right_only'\")[\"route_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate routes\n",
    "sub_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"route_id\",\n",
    "    \"direction_id\",\n",
    "    \"agency_id_gtfs\",\n",
    "    \"route_type\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    \"route_type_name\",\n",
    "    \"direction_name\",\n",
    "    \"agency_url\",\n",
    "    \"route\",\n",
    "]\n",
    "hs_route_dedup = hs_route_gdf.query(\"_merge == 'both'\").drop_duplicates(subset=sub_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish datasets to ArcGIS Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# checking if the directory demo_folder\n",
    "# exist or not.\n",
    "if not os.path.exists(\"Data\"):\n",
    "    # if the demo_folder directory is not present\n",
    "    # then create it.\n",
    "    os.makedirs(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export features to local directory\n",
    "point_path = os.path.join(\"Data\", \"hs_point_gdf.geojson\")\n",
    "hs_point_gdf.to_file(point_path, driver=\"GeoJSON\")\n",
    "\n",
    "line_path = os.path.join(\"Data\", \"hs_line_gdf.geojson\")\n",
    "hs_line_gdf.to_file(line_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote hosted feature layer with id: b03b3b392d324f2d828eaad56932e93b\n"
     ]
    }
   ],
   "source": [
    "# publish point features to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=point_path,\n",
    "    layer_name=\"BusAID Hotspots - Point (December 2023)\",\n",
    "    layer_snippet=\"BusAID Hotspot Point Dataset\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"b03b3b392d324f2d828eaad56932e93b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote hosted feature layer with id: c31d2e1c793f43c68ad79cc544453fe9\n"
     ]
    }
   ],
   "source": [
    "# publish line features to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=line_path,\n",
    "    layer_name=\"BusAID Hotspots - Line (December 2023)\",\n",
    "    layer_snippet=\"BusAID Hotspot Line Dataset\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"c31d2e1c793f43c68ad79cc544453fe9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop merge\n",
    "hs_route_dedup.drop(columns=[\"_merge\"], inplace=True)\n",
    "\n",
    "# export features to local directory\n",
    "hs_route_path = os.path.join(\"Data\", \"hotspot_bus_routes_gdf.geojson\")\n",
    "hs_route_dedup.to_file(hs_route_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote hosted feature layer with id: 46749db760bc4371b79f7ae223755b15\n"
     ]
    }
   ],
   "source": [
    "# publish routes near lines to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=hs_route_path,\n",
    "    layer_name=\"Hotspot Transit Routes\",\n",
    "    layer_snippet=\"This dataset represents transit routes that run through hotspots.\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"46749db760bc4371b79f7ae223755b15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay hotspot transit routes with epcs\n",
    "hs_epc_gdf = gpd.sjoin(\n",
    "    hs_route_dedup,\n",
    "    epc_gdf[[\"geoid\", \"epc_2050\", \"epc_class\", \"geometry\"]].to_crs(\"EPSG:26910\"),\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots deduplicating by id, geoid, epc_2020, and epc_class\n",
    "hs_epc_dedup = hs_epc_gdf[[\"hotspot_id\", \"geoid\", \"epc_2050\", \"epc_class\"]].drop_duplicates(\n",
    "    subset=[\"hotspot_id\", \"geoid\", \"epc_2050\", \"epc_class\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode pdas\n",
    "pda_explode_gdf = pda_gdf.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially join hotspot transit routes with pdas\n",
    "hs_pda_gdf = gpd.sjoin(\n",
    "    hs_route_dedup,\n",
    "    pda_explode_gdf[[\"join_key\", \"pda_name\", \"geometry\"]].to_crs(\"EPSG:26910\"),\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flag column to indicate if a hotspot is within a pda\n",
    "hs_pda_gdf[\"pda_flag\"] = np.where(hs_pda_gdf[\"join_key\"].isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots deduplicating by id, join_key, pda_name, and route_id\n",
    "hs_pda_dedup = (\n",
    "    hs_pda_gdf[[\"hotspot_id\", \"join_key\", \"pda_name\", \"route_id\", \"pda_flag\"]]\n",
    "    .sort_values(\"pda_flag\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"hotspot_id\", \"route_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize tracts by hotspot id, epc, and epc category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize tracts by hotspot, epc_2050, and epc_class\n",
    "hs_tract_summary = (\n",
    "    hs_epc_dedup.groupby([\"hotspot_id\", \"epc_2050\", \"epc_class\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total_census_tracts\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column and calculate percentage of total hotspot tracts\n",
    "# hs_tract_summary[\"pct_hs_total_tracts\"] =\n",
    "\n",
    "hs_tract_summary[\"pct_total_hs_tracts\"] = hs_tract_summary.groupby(\"hotspot_id\")[\n",
    "    \"total_census_tracts\"\n",
    "].transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "excel_path = os.path.join(dir, \"BusAID Hotspot Master List_112823.xlsx\")\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_tract_summary.to_excel(writer, sheet_name=\"hotspot_epc_summary\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize transit routes by hotspot id, and whether they intersect with pdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pda_rt_summary = (\n",
    "    hs_pda_dedup.groupby([\"hotspot_id\", \"pda_flag\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total_routes\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column and calculate the total number of routes that pass through pdas by the total number of routes that pass through hotspots\n",
    "hs_pda_rt_summary[\"pct_total_routes\"] = hs_pda_rt_summary.groupby(\"hotspot_id\")[\n",
    "    \"total_routes\"\n",
    "].transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_pda_rt_summary.to_excel(writer, sheet_name=\"hotspot_pda_summary\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test options for rows with only a single hotspot_id\n",
    "pda_melt = hs_pda_rt_summary.melt(id_vars=[\"hotspot_id\", \"pda_flag\"], value_vars=[\"total_routes\", \"pct_total_routes\"])\n",
    "pda_melt[\"new_variable\"] = pda_melt[\"variable\"] + \"_\" + pda_melt[\"pda_flag\"].astype(str)\n",
    "pda_melt.pivot(index=\"hotspot_id\", columns=\"new_variable\", values=\"value\").reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
