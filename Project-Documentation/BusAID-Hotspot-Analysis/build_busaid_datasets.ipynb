{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "DVUTILS_LOCAL_CLONE_PATH = f\"/Users/{user}/Documents/GitHub/dvutils\"\n",
    "sys.path.insert(0, DVUTILS_LOCAL_CLONE_PATH)\n",
    "from utils_io import *\n",
    "\n",
    "from arcgis import GIS\n",
    "from gtfs_functions import Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "dir = os.path.join(\"/Users\", user, \"Library\", \"CloudStorage\", \"Box-Box\", \"_GIS Analyses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to remove z values from a geometry\n",
    "def remove_z(geom):\n",
    "    \"\"\"\n",
    "    Removes z values from a geometry\n",
    "\n",
    "    Source: https://gist.github.com/rmania/8c88377a5c902dfbc134795a7af538d8?permalink_comment_id=2893099#gistcomment-2893099\n",
    "    \"\"\"\n",
    "    import shapely\n",
    "\n",
    "    return shapely.wkb.loads(shapely.wkb.dumps(geom, output_dimension=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to read kml files by geometry type and return a geodataframe\n",
    "def read_kml_by_geom_type(directory, geom_type):\n",
    "    \"\"\"Read a kml file by geometry type and return a geodataframe\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        path to the kml directory\n",
    "    geom_type : str\n",
    "        one of ['Point', 'LineString', 'Polygon']\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import glob\n",
    "    import fiona\n",
    "\n",
    "    fiona.supported_drivers[\"KML\"] = \"rw\"\n",
    "\n",
    "    # use glob to get all the csv files\n",
    "    # in the folder\n",
    "    if geom_type == \"Point\":\n",
    "        type_string = \"_point\"\n",
    "    elif geom_type == \"LineString\":\n",
    "        type_string = \"_segment\"\n",
    "    else:\n",
    "        type_string = \"_polygon\"\n",
    "\n",
    "    pattern = re.compile(type_string)\n",
    "    file_list = glob.glob(os.path.join(directory, \"*.kml\"))\n",
    "    gdfs = list()\n",
    "\n",
    "    for file in file_list:\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        if pattern.search(file_name):\n",
    "            # read the file\n",
    "            gdf = gpd.read_file(file, driver=\"KML\")\n",
    "            # add the source file name\n",
    "            gdf[\"source_file\"] = file_name\n",
    "            # remove z values\n",
    "            gdf[\"geometry\"] = gdf[\"geometry\"].apply(remove_z)\n",
    "            gdfs.append(gdf)\n",
    "    gdf = pd.concat(gdfs, ignore_index=True)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to overwrite a feature layer\n",
    "def overwrite_published_feature_layer(f_layer_id, geojson_path, client):\n",
    "    \"\"\"Overwrite a published feature layer\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    f_layer_id : str\n",
    "        id of the feature layer to overwrite\n",
    "    geojson_path : str\n",
    "        path to the geojson file\n",
    "    client : authenticated arcgis client\n",
    "        authentication example below:\n",
    "        from arcgis.gis import GIS\n",
    "        password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "        gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)\n",
    "    \"\"\"\n",
    "    from arcgis.features import FeatureLayerCollection\n",
    "\n",
    "    # get the feature layer\n",
    "    host_flayer = client.content.get(f_layer_id)\n",
    "\n",
    "    # create feature layer collection object\n",
    "    f_layer = FeatureLayerCollection.fromitem(host_flayer)\n",
    "    # overwrite the feature layer\n",
    "    f_layer.manager.overwrite(geojson_path)\n",
    "\n",
    "    print(f\"Overwrote hosted feature layer with id: {f_layer_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that publishes a geojson to agol\n",
    "def publish_geojson_to_agol(\n",
    "    geojson_path,\n",
    "    layer_name,\n",
    "    layer_snippet,\n",
    "    tags,\n",
    "    client,\n",
    "    folder=None,\n",
    "    overwrite=False,\n",
    "    f_layer_id=None,\n",
    "):\n",
    "    \"\"\"Publish a geojson to ArcGIS Online\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    geojson_path : str\n",
    "        path to the geojson file\n",
    "    layer_name : str\n",
    "        name of the layer\n",
    "    layer_snippet : str\n",
    "        layer snippet\n",
    "    tags : list\n",
    "        tags as a comma separated string (e.g. \"tag1, tag2, tag3\")\n",
    "    client : authenticated arcgis client\n",
    "        authentication example below:\n",
    "        from arcgis.gis import GIS\n",
    "        password = os.environ.get(\"AGOL_CONTENT_PASSWORD\")\n",
    "        gis = GIS(url=\"https://mtc.maps.arcgis.com/home/\", username=\"content_MTC\", password=password)\n",
    "    folder : str\n",
    "        name of the folder to publish to (optional)\n",
    "    overwrite : bool\n",
    "        if True, overwrite existing layer\n",
    "    f_layer_id : str\n",
    "        if overwrite is True, provide the id of the feature layer to overwrite\n",
    "    \"\"\"\n",
    "    if overwrite:\n",
    "        overwrite_published_feature_layer(f_layer_id, geojson_path, client)\n",
    "    else:\n",
    "        # publish the geojson\n",
    "        item_prop = {\n",
    "            \"type\": \"GeoJson\",\n",
    "            \"title\": layer_name,\n",
    "            \"tags\": tags,\n",
    "            \"snippet\": layer_snippet,\n",
    "            \"overwrite\": True,\n",
    "        }\n",
    "        item = client.content.add(item_properties=item_prop, data=geojson_path, folder=folder)\n",
    "\n",
    "        # publish the item\n",
    "        published_item = item.publish(file_type=\"geojson\")\n",
    "\n",
    "        print(f\"Published {layer_name} to ArcGIS Online as {published_item.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flag_column(flag_gdf, original_gdf, original_id_col, out_column):\n",
    "    if flag_gdf.shape[0] != original_gdf.shape[0]:\n",
    "        original_gdf[out_column] = original_gdf[original_id_col].map(\n",
    "            flag_gdf.groupby(original_id_col)[out_column].first()\n",
    "        )\n",
    "    else:\n",
    "        original_gdf[out_column] = original_gdf[original_id_col].map(\n",
    "            flag_gdf.set_index(original_id_col)[out_column]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list_values(row):\n",
    "    row_list = list(row)\n",
    "    row_list.sort()\n",
    "    list_set = set(row_list)\n",
    "    unique_list = list(list_set)\n",
    "    return \"; \".join(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and pre-process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel master list\n",
    "hs_ms_df = pd.read_excel(\n",
    "    os.path.join(dir, \"BusAID Hotspot Master List_112823.xlsx\"), sheet_name=\"hotspot_data_revised\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking feature service layer IDs into 8 chunks\n"
     ]
    }
   ],
   "source": [
    "# read epcs\n",
    "epc_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/communities_of_concern_2020_acs2018/FeatureServer/0\"\n",
    "epc_gdf = pull_geotable_agol(base_url=epc_url, client=gis, reproject_to_analysis_crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking feature service layer IDs into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# read pdas\n",
    "pda_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/priority_development_areas_pba2050/FeatureServer/0\"\n",
    "pda_gdf = pull_geotable_agol(base_url=pda_url, client=gis, reproject_to_analysis_crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null id\n",
    "hs_ms_df = hs_ms_df[~hs_ms_df[\"hotspot_id\"].isnull()].copy()\n",
    "# set id as integer\n",
    "hs_ms_df[\"hotspot_id\"] = hs_ms_df[\"hotspot_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the following hotspots from the analysis:\n",
    "# - Remove hotspots #108, 109, 110 (WestCAT-identified hotspots) and any routes that only pass through these hotspots. These are being addressed via a DPD Forwards project.\n",
    "# - Remove hotspots #82, 84, 85 (SFMTA-identified hotspots) and any routes that only pass through these hotspots. SFMTA has decided to withdraw these from consideration for the BusAID program.\n",
    "\n",
    "hs_ms_df = hs_ms_df[~hs_ms_df[\"hotspot_id\"].isin([108, 109, 110, 82, 84, 85])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point and line gdfs\n",
    "point_gdf = read_kml_by_geom_type(os.path.join(dir, \"Spatial Data\"), \"Point\")\n",
    "line_gdf = read_kml_by_geom_type(os.path.join(dir, \"Spatial Data\"), \"LineString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the id from name string column and add to a new column\n",
    "line_gdf[\"hotspot_id\"] = line_gdf[\"Name\"].str.extract(r\"^\\((\\d+)\\)\", expand=False).astype(int)\n",
    "point_gdf[\"hotspot_id\"] = point_gdf[\"Name\"].str.extract(r\"^\\((\\d+)\\)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge master list with point and line gdfs\n",
    "hs_point_gdf = pd.merge(point_gdf[[\"hotspot_id\", \"geometry\"]], hs_ms_df, on=\"hotspot_id\", how=\"inner\")\n",
    "hs_line_gdf = pd.merge(line_gdf[[\"hotspot_id\", \"geometry\"]], hs_ms_df, on=\"hotspot_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots exploding comma separated route ids into multiple rows\n",
    "hs_route_df = hs_ms_df[[\"hotspot_id\", \"agency\", \"transit_routes\"]].copy()\n",
    "\n",
    "# remove whitespace from transit routes\n",
    "hs_route_df[\"transit_routes\"] = hs_route_df[\"transit_routes\"].astype(str).str.replace(\" \", \"\")\n",
    "\n",
    "# split transit routes into list\n",
    "hs_route_df[\"route\"] = hs_route_df.transit_routes.str.split(\",\")\n",
    "\n",
    "# add agency id column to hotspot route dataframe\n",
    "agency_id_dict = {\n",
    "    \"AC Transit\": \"AC\",\n",
    "    \"BART\": \"BA\",\n",
    "    \"CityBus\": \"SR\",\n",
    "    \"County Connection\": \"CC\",\n",
    "    # \"Dixon Readi-Ride\": \"\", # not in transit routes\n",
    "    \"FAST\": \"FS\",\n",
    "    \"Golden Gate Transit\": \"GG\",\n",
    "    \"LAVTA\": \"WH\",\n",
    "    \"Marin Transit\": \"MA\",\n",
    "    \"NVTA\": \"VN\",\n",
    "    \"Petaluma Transit\": \"PE\",\n",
    "    \"SamTrans\": \"SM\",\n",
    "    \"SFMTA\": \"SF\",\n",
    "    \"SolTrans\": \"ST\",\n",
    "    \"Sonoma County Transit\": \"SO\",\n",
    "    \"Tri Delta\": \"3D\",\n",
    "    \"Union City Transit\": \"UC\",\n",
    "    \"VTA\": \"SC\",\n",
    "    \"WestCAT\": \"WC\",\n",
    "}\n",
    "hs_route_df[\"agency_id\"] = hs_route_df[\"agency\"].map(agency_id_dict)\n",
    "\n",
    "# explode the route column into multiple rows\n",
    "hs_route_expode_df = hs_route_df[[\"hotspot_id\", \"agency\", \"agency_id\", \"route\"]].explode(\"route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_route_expode_df[\"route_id\"] = hs_route_expode_df[\"agency_id\"] + \":\" + hs_route_expode_df[\"route\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update route_id for routes that have a different id in the transit routes table\n",
    "route_dict = {\n",
    "    \"BA:Vine29\": \"VN:29\",\n",
    "    \"BA:SolTransG\": \"ST:G\",\n",
    "    \"BA:R\": \"ST:R\",\n",
    "    \"BA:GGT580\": \"GG:580\",\n",
    "    \"BA:704\": \"GG:704\",\n",
    "    \"BA:AC72R\": \"AC:72R\",\n",
    "    \"BA:72\": \"AC:72\",\n",
    "    \"BA:72M\": \"AC:72M\",\n",
    "    \"BA:800\": \"AC:800\",\n",
    "    \"BA:7\": \"AC:7\",\n",
    "    \"BA:76\": \"AC:76\",\n",
    "    \"BA:376\": \"AC:376\",\n",
    "    \"BA:L\": \"AC:L\",\n",
    "    \"BA:WestCATJK\": \"WC:JX\",\n",
    "    \"BA:JPX\": \"WC:JPX\",\n",
    "    \"BA:JL\": \"WC:J\",\n",
    "    \"BA:JR\": \"WC:J\",\n",
    "    \"SR:1\": \"SR:01\",\n",
    "    \"SR:2\": \"SR:02\",\n",
    "    \"SR:2B\": \"SR:02B\",\n",
    "    \"SR:3\": \"SR:03\",\n",
    "    \"SR:5\": \"SR:05\",\n",
    "    \"SR:4\": \"SR:04\",\n",
    "    \"SR:4B\": \"SR:04B\",\n",
    "    \"SR:6\": \"SR:06\",\n",
    "    \"SR:8\": \"SR:08\",\n",
    "    \"SR:9\": \"SR:09\",\n",
    "    \"SR:20\": \"SO:20\",\n",
    "    \"SR:30\": \"SO:30\",\n",
    "    \"SR:34\": \"SO:34\",\n",
    "    \"SR:46\": \"SO:46\",\n",
    "    \"SR:44\": \"SO:44\",\n",
    "    \"SR:48\": \"SO:48\",\n",
    "    \"SR:60\": \"SO:60\",\n",
    "    \"SR:62\": \"SO:62\",\n",
    "    \"SR:72\": \"GG:72\",\n",
    "    # \"SR:95\": None,\n",
    "    \"SR:101\": \"GG:101\",\n",
    "    # \"FS:microtransit\": None,\n",
    "    # \"PE:10\": None,\n",
    "    # \"PE:501\": None,\n",
    "    \"SM:ECROwl\": \"SM:ECRO\",\n",
    "    \"SM:296Owl\": \"SM:296O\",\n",
    "    # \"SF:K\": None,\n",
    "    \"SO:101(GGT)\": \"GG:101\",\n",
    "    # \"SO:95\": None,\n",
    "    \"UC:41\": \"AC:41\",\n",
    "    \"UC:56\": \"AC:56\",\n",
    "    \"UC:97\": \"AC:97\",\n",
    "    \"UC:210\": \"AC:210\",\n",
    "    # \"SC:500\": None,\n",
    "}\n",
    "hs_route_expode_df[\"route_id\"] = hs_route_expode_df[\"route_id\"].replace(route_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish datasets to ArcGIS Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# checking if the directory demo_folder\n",
    "# exist or not.\n",
    "if not os.path.exists(\"Data\"):\n",
    "    # if the demo_folder directory is not present\n",
    "    # then create it.\n",
    "    os.makedirs(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export features to local directory\n",
    "point_path = os.path.join(\"Data\", \"hs_point_gdf.geojson\")\n",
    "hs_point_gdf.to_file(point_path, driver=\"GeoJSON\")\n",
    "\n",
    "line_path = os.path.join(\"Data\", \"hs_line_gdf.geojson\")\n",
    "hs_line_gdf.to_file(line_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish point features to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=point_path,\n",
    "    layer_name=\"BusAID Hotspots - Point (December 2023)\",\n",
    "    layer_snippet=\"BusAID Hotspot Point Dataset\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"b03b3b392d324f2d828eaad56932e93b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish line features to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=line_path,\n",
    "    layer_name=\"BusAID Hotspots - Line (December 2023)\",\n",
    "    layer_snippet=\"BusAID Hotspot Line Dataset\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"c31d2e1c793f43c68ad79cc544453fe9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop merge\n",
    "hs_route_dedup.drop(columns=[\"_merge\"], inplace=True)\n",
    "\n",
    "# export features to local directory\n",
    "hs_route_path = os.path.join(\"Data\", \"hotspot_bus_routes_gdf.geojson\")\n",
    "hs_route_dedup.to_file(hs_route_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish routes near lines to agol\n",
    "publish_geojson_to_agol(\n",
    "    geojson_path=hs_route_path,\n",
    "    layer_name=\"Hotspot Transit Routes\",\n",
    "    layer_snippet=\"This dataset represents transit routes that run through hotspots.\",\n",
    "    tags=\"mtc, bay area, busaid, bus, transit, hotspots\",\n",
    "    client=gis,\n",
    "    folder=\"bus_aid\",\n",
    "    overwrite=True,\n",
    "    f_layer_id=\"46749db760bc4371b79f7ae223755b15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process GTFS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read transit routes\n",
    "# transit_url = \"https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/511_Transit_Routes_Sep_2023/FeatureServer/0\"\n",
    "# transit_gdf = pull_geotable_agol(base_url=transit_url, client=gis, reproject_to_analysis_crs=False)\n",
    "api_key = os.environ.get(\"GTFS_API_KEY\")\n",
    "gtfs_path = (\n",
    "    f\"http://api.511.org/transit/datafeeds?api_key={api_key}&operator_id=RG&historic=2023-11\"\n",
    ")\n",
    "feed = Feed(\n",
    "    gtfs_path,\n",
    "    time_windows=[0, 6, 10, 15, 19, 22, 24],\n",
    "    busiest_date=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/gtfs_functions/gtfs_functions.py\u001b[0m in \u001b[0;36mextract_file\u001b[0;34m(file, feed)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgtfs_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Reading \"{file}.txt\".'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'http://api.511.org/transit/datafeeds?api_key=7fc8acf3-74f7-4120-ba90-247f569550c2&operator_id=RG&historic=2023-11'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9q/xt2lctm54xq6fd45m1lmgp4m0000gp/T/ipykernel_34446/3421092528.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create line frequency and route dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# line_freq = feed.lines_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mroutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0magency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/gtfs_functions/gtfs_functions.py\u001b[0m in \u001b[0;36mroutes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mroutes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_routes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_routes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_routes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_routes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/gtfs_functions/gtfs_functions.py\u001b[0m in \u001b[0;36mget_routes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_routes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mroutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'routes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0mroutes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'route_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroutes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/gtfs_functions/gtfs_functions.py\u001b[0m in \u001b[0;36mextract_file\u001b[0;34m(file, feed)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34mf'{file}.txt'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgtfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Reading \"{file}.txt\".'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 decoded = self._decode(\n\u001b[1;32m    833\u001b[0m                     \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mreturned_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mIncompleteRead\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdetect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/busaid_env/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create line frequency and route dataframes\n",
    "# line_freq = feed.lines_freq\n",
    "routes = feed.routes\n",
    "agency = feed.agency\n",
    "shapes = feed.shapes\n",
    "trips = feed.trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes together to get route shapes with agency names\n",
    "st_gdf = pd.merge(\n",
    "    shapes, trips[[\"trip_id\", \"route_id\", \"direction_id\", \"shape_id\"]], on=\"shape_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "str_gdf = pd.merge(\n",
    "    st_gdf,\n",
    "    routes[[\"route_id\", \"agency_id\", \"route_name\", \"route_type\"]],\n",
    "    on=\"route_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "stra_gdf = pd.merge(\n",
    "    str_gdf,\n",
    "    agency[[\"agency_id\", \"agency_name\", \"agency_url\"]],\n",
    "    on=\"agency_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# reorder columns\n",
    "cols = [\n",
    "    \"route_id\",\n",
    "    \"agency_id\",\n",
    "    \"trip_id\",\n",
    "    \"direction_id\",\n",
    "    \"shape_id\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    \"route_type\",\n",
    "    \"agency_url\",\n",
    "    \"geometry\",\n",
    "]\n",
    "route_gdf = stra_gdf[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all expected agency_ids are within the line_freq dataframe\n",
    "expected_agency_ids = [\n",
    "    \"AC\",\n",
    "    \"BA\",\n",
    "    \"SR\",\n",
    "    \"CC\",\n",
    "    \"FS\",\n",
    "    \"GG\",\n",
    "    \"WH\",\n",
    "    \"MA\",\n",
    "    \"VN\",\n",
    "    \"PE\",\n",
    "    \"SM\",\n",
    "    \"SF\",\n",
    "    \"ST\",\n",
    "    \"SO\",\n",
    "    \"3D\",\n",
    "    \"UC\",\n",
    "    \"SC\",\n",
    "    \"WC\",\n",
    "]\n",
    "line_freq_agency_ids = route_gdf[\"agency_id\"].unique().tolist()\n",
    "missing_agency_ids = []\n",
    "for id in expected_agency_ids:\n",
    "    if id not in line_freq_agency_ids:\n",
    "        missing_agency_ids.append(id)\n",
    "missing_agency_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add human readable route type and direction\n",
    "route_type_dict = route_type_dict = {\n",
    "    0: \"Tram, Streetcar, Light Rail\",\n",
    "    1: \"Subway, Metro\",\n",
    "    2: \"Rail\",\n",
    "    3: \"Bus\",\n",
    "    4: \"Ferry\",\n",
    "    5: \"Cable Tram\",\n",
    "    6: \"Aerial Lift\",\n",
    "    7: \"Funicular\",\n",
    "    11: \"Trollybus\",\n",
    "    12: \"Monorail\",\n",
    "}\n",
    "route_gdf[\"route_type_name\"] = route_gdf[\"route_type\"].map(route_type_dict)\n",
    "route_gdf[\"direction_name\"] = route_gdf[\"direction_id\"].map({0: \"Outbound\", 1: \"Inbound\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "cols = [\n",
    "    \"route_id\",\n",
    "    \"agency_id\",\n",
    "    \"direction_id\",\n",
    "    \"route_type\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    # \"route_short_name\",\n",
    "    \"route_type_name\",\n",
    "    \"direction_name\",\n",
    "    # \"window\",\n",
    "    # \"min_per_trip\",\n",
    "    # \"ntrips\",\n",
    "    \"agency_url\",\n",
    "    \"geometry\",\n",
    "]\n",
    "route_gdf = route_gdf[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter transit route data to only include routes with the following conditions:\n",
    "# - route_type = 3 (bus)\n",
    "# - window = \"6:00-10:00\" (AM peak)\n",
    "# - dir_id = \"Inbound\"\n",
    "\n",
    "# bus_gdf = transit_gdf.query(\"route_type == 3 and window == '6:00-10:00' and dir_id == 'Inbound'\").copy().to_crs(\"EPSG:26910\")\n",
    "bus_lr_gdf = (\n",
    "    route_gdf.query(\"route_type_name.isin(['Bus', 'Tram, Streetcar, Light Rail'])\")\n",
    "    .copy()\n",
    "    .to_crs(\"EPSG:26910\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge hotspot spatial and tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the route id with the bus transit gdf\n",
    "hs_route_gdf = pd.merge(\n",
    "    bus_lr_gdf,\n",
    "    hs_route_expode_df,\n",
    "    on=\"route_id\",\n",
    "    how=\"right\",\n",
    "    suffixes=[\"_gtfs\", \"_hotspot\"],\n",
    "    indicator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_route_gdf[\"_merge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_route_gdf.query(\"_merge == 'right_only'\")[\"route_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate routes\n",
    "sub_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"route_id\",\n",
    "    \"direction_id\",\n",
    "    \"agency_id_gtfs\",\n",
    "    \"route_type\",\n",
    "    \"agency_name\",\n",
    "    \"route_name\",\n",
    "    \"route_type_name\",\n",
    "    \"direction_name\",\n",
    "    \"agency_url\",\n",
    "    \"route\",\n",
    "]\n",
    "hs_route_dedup = hs_route_gdf.query(\"_merge == 'both'\").drop_duplicates(subset=sub_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay hotspot transit routes with epcs\n",
    "epc_cols = [\n",
    "    \"geoid\",\n",
    "    \"tot_pop\",\n",
    "    \"tot_pop_po\",\n",
    "    \"tot_pop_ci\",\n",
    "    \"tot_hh\",\n",
    "    \"tot_fam\",\n",
    "    \"tot_pop_ov\",\n",
    "    \"pop_poc\",\n",
    "    \"pop_over75\",\n",
    "    \"pop_spfam\",\n",
    "    \"pop_lep\",\n",
    "    \"pop_below2\",\n",
    "    \"pop_disabi\",\n",
    "    \"pop_hus_re\",\n",
    "    \"pop_zvhhs\",\n",
    "    \"epc_2050\",\n",
    "    \"epc_class\",\n",
    "    \"geometry\",\n",
    "]\n",
    "hs_epc_gdf = gpd.sjoin(\n",
    "    hs_route_dedup,\n",
    "    epc_gdf[epc_cols].to_crs(\"EPSG:26910\"),\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots deduplicating by id, geoid, epc_2020, and epc_class\n",
    "hs_epc_dedup = hs_epc_gdf.drop_duplicates(\n",
    "    subset=[\"hotspot_id\", \"geoid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode pdas\n",
    "pda_explode_gdf = pda_gdf.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially join hotspot transit routes with pdas\n",
    "hs_pda_gdf = gpd.sjoin(\n",
    "    hs_route_dedup,\n",
    "    pda_explode_gdf[[\"join_key\", \"pda_name\", \"geometry\"]].to_crs(\"EPSG:26910\"),\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flag column to indicate if a hotspot is within a pda\n",
    "hs_pda_gdf[\"pda_flag\"] = np.where(hs_pda_gdf[\"join_key\"].isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of hotspots deduplicating by id, join_key, pda_name, and route_id\n",
    "hs_pda_dedup = (\n",
    "    hs_pda_gdf\n",
    "    .sort_values(\"pda_flag\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"hotspot_id\", \"route_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize tracts by hotspot id, epc, and epc category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize tracts by hotspot, epc_2050, and epc_class\n",
    "hs_tract_summary = (\n",
    "    hs_epc_dedup.groupby([\"hotspot_id\", \"epc_class\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total_census_tracts\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column and calculate percentage of total hotspot tracts\n",
    "# hs_tract_summary[\"pct_hs_total_tracts\"] =\n",
    "\n",
    "hs_tract_summary[\"pct_total_hs_tracts\"] = hs_tract_summary.groupby(\"hotspot_id\")[\n",
    "    \"total_census_tracts\"\n",
    "].transform(lambda x: round(x / x.sum(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table so hotspot ids are rows and epc classes are columns\n",
    "epc_melt = hs_tract_summary.melt(\n",
    "    id_vars=[\"hotspot_id\", \"epc_class\"], value_vars=[\"total_census_tracts\", \"pct_total_hs_tracts\"]\n",
    ")\n",
    "\n",
    "# create a new variable column\n",
    "epc_melt[\"new_variable\"] = epc_melt[\"variable\"] + \"_\" + epc_melt[\"epc_class\"]\n",
    "\n",
    "# pivot table so hotspot ids are rows and epc classes are columns\n",
    "hs_tract_pivot = epc_melt.pivot_table(\n",
    "    values=\"value\", index=\"hotspot_id\", columns=\"new_variable\", aggfunc=\"max\"\n",
    ").reset_index()\n",
    "\n",
    "# remove index name\n",
    "hs_tract_pivot = hs_tract_pivot.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "hs_tract_pivot.rename(\n",
    "    columns={\n",
    "        \"pct_total_hs_tracts_High\": \"pct_tracts_high\",\n",
    "        \"pct_total_hs_tracts_Higher\": \"pct_tracts_higher\",\n",
    "        \"pct_total_hs_tracts_Highest\": \"pct_tracts_highest\",\n",
    "        \"pct_total_hs_tracts_NA\": \"pct_tracts_not_epc\",\n",
    "        \"total_census_tracts_High\": \"count_tracts_high\",\n",
    "        \"total_census_tracts_Higher\": \"count_tracts_higher\",\n",
    "        \"total_census_tracts_Highest\": \"count_tracts_highest\",\n",
    "        \"total_census_tracts_NA\": \"count_tracts_not_epc\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add qaqc columns to total census tracts and percent total census tracts\n",
    "pct_cols = [\"pct_tracts_high\", \"pct_tracts_higher\", \"pct_tracts_highest\", \"pct_tracts_not_epc\"]\n",
    "hs_tract_pivot[\"pct_total\"] = hs_tract_pivot[pct_cols].sum(axis=1)\n",
    "\n",
    "count_cols = [\n",
    "    \"count_tracts_high\",\n",
    "    \"count_tracts_higher\",\n",
    "    \"count_tracts_highest\",\n",
    "    \"count_tracts_not_epc\",\n",
    "]\n",
    "hs_tract_pivot[\"total_tracts\"] = hs_tract_pivot[count_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"pct_tracts_high\",\n",
    "    \"pct_tracts_higher\",\n",
    "    \"pct_tracts_highest\",\n",
    "    \"pct_tracts_not_epc\",\n",
    "    \"pct_total\",\n",
    "    \"count_tracts_high\",\n",
    "    \"count_tracts_higher\",\n",
    "    \"count_tracts_highest\",\n",
    "    \"count_tracts_not_epc\",\n",
    "    \"total_tracts\",\n",
    "]\n",
    "hs_tract_pivot = hs_tract_pivot[reorder_cols].copy()\n",
    "\n",
    "# fill na values with 0\n",
    "hs_tract_pivot.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "excel_path = os.path.join(dir, \"BusAID Hotspot Master List_112823.xlsx\")\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_tract_pivot.to_excel(writer, sheet_name=\"hotspot_epc_summary\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize transit routes by hotspot id, and whether they intersect with pdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pda_rt_summary = (\n",
    "    hs_pda_dedup.groupby([\"hotspot_id\", \"pda_flag\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total_routes\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column and calculate the total number of routes that pass through pdas by the total number of routes that pass through hotspots\n",
    "hs_pda_rt_summary[\"pct_total_routes\"] = hs_pda_rt_summary.groupby(\"hotspot_id\")[\n",
    "    \"total_routes\"\n",
    "].transform(lambda x: round(x / x.sum(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test options for rows with only a single hotspot_id\n",
    "pda_melt = hs_pda_rt_summary.melt(id_vars=[\"hotspot_id\", \"pda_flag\"], value_vars=[\"total_routes\", \"pct_total_routes\"])\n",
    "pda_melt[\"new_variable\"] = pda_melt[\"variable\"] + \"_\" + pda_melt[\"pda_flag\"].astype(str)\n",
    "hs_pda_rt_pivot = pda_melt.pivot(index=\"hotspot_id\", columns=\"new_variable\", values=\"value\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pda_rt_pivot.rename(\n",
    "    columns={\n",
    "        \"pct_total_routes_0\": \"pct_routes_outside_pda\",\n",
    "        \"pct_total_routes_1\": \"pct_routes_inside_pda\",\n",
    "        \"total_routes_0\": \"count_routes_outside_pda\",\n",
    "        \"total_routes_1\": \"count_routes_inside_pda\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add qaqc columns to total routes and percent total routes\n",
    "pct_cols = [\"pct_routes_outside_pda\", \"pct_routes_inside_pda\"]\n",
    "hs_pda_rt_pivot[\"pct_total\"] = hs_pda_rt_pivot[pct_cols].sum(axis=1)\n",
    "\n",
    "count_cols = [\"count_routes_outside_pda\", \"count_routes_inside_pda\"]\n",
    "hs_pda_rt_pivot[\"total_routes\"] = hs_pda_rt_pivot[count_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "reorder_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"pct_routes_outside_pda\",\n",
    "    \"pct_routes_inside_pda\",\n",
    "    \"pct_total\",\n",
    "    \"count_routes_outside_pda\",\n",
    "    \"count_routes_inside_pda\",\n",
    "    \"total_routes\",\n",
    "]\n",
    "\n",
    "hs_pda_rt_pivot = hs_pda_rt_pivot[reorder_cols].copy()\n",
    "\n",
    "# fill na values with 0\n",
    "hs_pda_rt_pivot.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index name\n",
    "hs_pda_rt_pivot = hs_pda_rt_pivot.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_pda_rt_pivot.to_excel(writer, sheet_name=\"hotspot_pda_summary\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize demographic data by hotspot id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize tracts by hotspot, epc_2050, and epc_class\n",
    "out_cols = [\n",
    "    \"hotspot_id\",\n",
    "    \"geoid\",\n",
    "    \"tot_pop\",\n",
    "    \"tot_pop_po\",\n",
    "    \"tot_pop_ci\",\n",
    "    \"tot_hh\",\n",
    "    \"tot_fam\",\n",
    "    \"tot_pop_ov\",\n",
    "    \"pop_poc\",\n",
    "    \"pop_over75\",\n",
    "    \"pop_spfam\",\n",
    "    \"pop_lep\",\n",
    "    \"pop_below2\",\n",
    "    \"pop_disabi\",\n",
    "    \"pop_hus_re\",\n",
    "    \"pop_zvhhs\",\n",
    "]\n",
    "\n",
    "hs_demo_summary = (\n",
    "    hs_epc_dedup.groupby([\"hotspot_id\"], dropna=False)[out_cols]\n",
    "    .agg(\n",
    "        total_tracts=(\"geoid\", \"count\"),\n",
    "        total_population=(\"tot_pop\", \"sum\"),\n",
    "        total_population_poverty=(\"tot_pop_po\", \"sum\"),\n",
    "        total_population_civilian=(\"tot_pop_ci\", \"sum\"),\n",
    "        total_households=(\"tot_hh\", \"sum\"),\n",
    "        total_families=(\"tot_fam\", \"sum\"),\n",
    "        total_population_over5=(\"tot_pop_ov\", \"sum\"),\n",
    "        pop_people_of_color=(\"pop_poc\", \"sum\"),\n",
    "        pop_seniors_ov75=(\"pop_over75\", \"sum\"),\n",
    "        pop_single_parent_families=(\"pop_spfam\", \"sum\"),\n",
    "        pop_limited_english_prof=(\"pop_lep\", \"sum\"),\n",
    "        pop_low_income=(\"pop_below2\", \"sum\"),\n",
    "        pop_disability=(\"pop_disabi\", \"sum\"),\n",
    "        pop_rent_burdened=(\"pop_hus_re\", \"sum\"),\n",
    "        pop_zero_vehicle_hhs=(\"pop_zvhhs\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate population percentages\n",
    "hs_demo_summary[\"pct_people_of_color\"] = np.where(\n",
    "    hs_demo_summary[\"total_population\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_people_of_color\"] / hs_demo_summary[\"total_population\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_seniors_ov75\"] = np.where(\n",
    "    hs_demo_summary[\"total_population\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_seniors_ov75\"] / hs_demo_summary[\"total_population\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_single_parent_families\"] = np.where(\n",
    "    hs_demo_summary[\"total_families\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_single_parent_families\"] / hs_demo_summary[\"total_families\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_limited_english_prof\"] = np.where(\n",
    "    hs_demo_summary[\"total_population_over5\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_limited_english_prof\"] / hs_demo_summary[\"total_population_over5\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_low_income\"] = np.where(\n",
    "    hs_demo_summary[\"total_population_poverty\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_low_income\"] / hs_demo_summary[\"total_population_poverty\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_disability\"] = np.where(\n",
    "    hs_demo_summary[\"total_population_civilian\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_disability\"] / hs_demo_summary[\"total_population_civilian\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_zero_vehicle_hhs\"] = np.where(\n",
    "    hs_demo_summary[\"total_households\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_zero_vehicle_hhs\"] / hs_demo_summary[\"total_households\"]), 3),\n",
    ")\n",
    "hs_demo_summary[\"pct_rent_burdened\"] = np.where(\n",
    "    hs_demo_summary[\"total_households\"] == 0,\n",
    "    0,\n",
    "    round((hs_demo_summary[\"pop_rent_burdened\"] / hs_demo_summary[\"total_households\"]), 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write summary results to existing excel document\n",
    "with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    hs_demo_summary.to_excel(writer, sheet_name=\"hotspot_demographic_summary\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
